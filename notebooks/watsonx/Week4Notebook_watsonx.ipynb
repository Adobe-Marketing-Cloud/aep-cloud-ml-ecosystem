{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "dcf0f9c8-8006-4473-a367-3d292c68cd79"
   },
   "source": [
    "# Scope of Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80b5ed9c40524d4fbcd135c4d13f9920"
   },
   "source": [
    "The goal of this notebook is to showcase how you can use, in your own environment, a pre-trained model along with some profile data extracted from the Adobe Experience Platform to generate propensity scores and ingest those back to enrich the Unified Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "![EndToEndDesign](images/CMLE-Notebooks-Week4-Workflow-Watsonx.png)"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485626012c1c42968ea26d47af27f099"
   },
   "source": [
    "We'll go through several steps:\n",
    "- **Reading the featurized data** from the Data Landing Zone\n",
    "- Generating the __scores__\n",
    "- Creating a __target dataset__\n",
    "- Creating a __dataflow__ to deliver data in the right format to that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f078631397d943a0bf4f604204579888"
   },
   "source": [
    "# Setup\n",
    "This notebook requires some configuration data to properly authenticate to your Adobe Experience Platform instance. You should be able to find all the values required above by following the Setup section of the __README__.\n",
    "The next cell will be looking for your configuration file under your project assets to fetch the values used throughout this notebook. See more details in the __Setup__ section of the __README__ to understand how to create your configuration file.\n",
    "NOTE: ensure the cluster type for this notebook is set to (Python 3.10 with Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "545b0c5977d34c06861c3e92443e23df"
   },
   "outputs": [],
   "source": [
    "from project_lib import Project\n",
    "from configparser import ConfigParser\n",
    "import io\n",
    "\n",
    "project = Project.access()\n",
    "config_file = project.get_file('config.ini')\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read_string(config_file.read().decode('utf-8'))\n",
    "\n",
    "ims_org_id = config.get(\"Platform\", \"ims_org_id\")\n",
    "sandbox_name = config.get(\"Platform\", \"sandbox_name\")\n",
    "environment = config.get(\"Platform\", \"environment\")\n",
    "client_id = config.get(\"Authentication\", \"client_id\")\n",
    "client_secret = config.get(\"Authentication\", \"client_secret\")\n",
    "scopes = config.get(\"Authentication\", \"scopes\")\n",
    "dataset_id = config.get(\"Platform\", \"dataset_id\")\n",
    "featurized_dataset_id = config.get(\"Platform\", \"featurized_dataset_id\")\n",
    "export_path = config.get(\"Cloud\", \"export_path\")\n",
    "import_path = config.get(\"Cloud\", \"import_path\")\n",
    "data_format = config.get(\"Cloud\", \"data_format\")\n",
    "compression_type = config.get(\"Cloud\", \"compression_type\")\n",
    "model_name = config.get(\"Cloud\", \"model_name\")\n",
    "\n",
    "watson_username = config.get(\"Watsonx\", \"watson_username\")\n",
    "watson_apikey = config.get(\"Watsonx\", \"watson_apikey\")\n",
    "model_id = config.get(\"Watsonx\", \"model_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d682ea707c747708ec6f518bf111a3e"
   },
   "source": [
    "Some utility functions that will be used throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ddeaf39ab6904a81841a8f842c8efd94"
   },
   "outputs": [],
   "source": [
    "def get_ui_link(tenant_id, resource_type, resource_id):\n",
    "    if environment == \"prod\":\n",
    "        prefix = f\"https://experience.adobe.com\"\n",
    "    else:\n",
    "        prefix = f\"https://experience-{environment}.adobe.com\"\n",
    "    return f\"{prefix}/#/@{tenant_id}/sname:{sandbox_name}/platform/{resource_type}/{resource_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6d2b5d057ab463789229dc18c976b7b"
   },
   "source": [
    "To ensure uniqueness of resources created as part of this notebook, we are using your system provisioned username to include in each of the resource titles to avoid conflicts, it is __recommended to supply a more readable__ one so you could easily identify resources in AEP created by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6e46939cda5a45388acc6bee92455d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: mndymuqvx34peqwqz-ydi68gcdn1kj9ugzqs-towtum\n",
      "Unique ID: mndymuqvx34peqwqz_ydi68gcdn1kj9ugzqs_towtum\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "username=watson_username # supply your custom one ex: foo@bar.com\n",
    "unique_id = s = re.sub(\"[^0-9a-zA-Z]+\", \"_\", watson_username)\n",
    "\n",
    "print(f\"Username: {username}\")\n",
    "print(f\"Unique ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d769430e36f48eead4976a41d3fe7e9"
   },
   "source": [
    "Before we run anything, make sure to install the following required libraries for this notebook. They are all publicly available libraries and the latest version should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0841ed497ca0412e8e4e1b61ab6e889d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.12.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (2.2.1)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (0.4)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (6.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (3.3.4)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (3.7.1)\n",
      "Requirement already satisfied: numpy<2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (1.23.5)\n",
      "Requirement already satisfied: packaging<25 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (23.0)\n",
      "Requirement already satisfied: pandas<3 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (1.4.3)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (4.21.12)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (11.0.0)\n",
      "Requirement already satisfied: pytz<2025 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (2022.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (6.0)\n",
      "Collecting querystring-parser<2 (from mlflow)\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (1.1.1)\n",
      "Requirement already satisfied: scipy<2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (1.10.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (1.4.52)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow)\n",
      "  Downloading sqlparse-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from mlflow) (3.1.3)\n",
      "Collecting gunicorn<23 (from mlflow)\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.4.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting blinker>=1.6.2 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading mlflow-2.12.2-py3-none-any.whl (20.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m825.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m870.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: aniso8601, Werkzeug, sqlparse, smmap, querystring-parser, Mako, itsdangerous, gunicorn, graphql-core, click, blinker, graphql-relay, gitdb, Flask, docker, alembic, graphene, gitpython, mlflow\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.3.8\n",
      "    Uninstalling Werkzeug-2.3.8:\n",
      "      Successfully uninstalled Werkzeug-2.3.8\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.4\n",
      "    Uninstalling blinker-1.4:\n",
      "      Successfully uninstalled blinker-1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lale 0.7.7 requires click==8.0.4, but you have click 8.1.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 Mako-1.3.5 Werkzeug-3.0.3 alembic-1.13.1 aniso8601-9.0.1 blinker-1.8.2 click-8.1.7 docker-7.0.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 itsdangerous-2.2.0 mlflow-2.12.2 querystring-parser-1.2.4 smmap-5.0.1 sqlparse-0.5.0\n",
      "Collecting aepp\n",
      "  Downloading aepp-0.3.4.post2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pandas in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aepp) (1.4.3)\n",
      "Requirement already satisfied: requests in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aepp) (2.31.0)\n",
      "Requirement already satisfied: PyJWT in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aepp) (2.4.0)\n",
      "Collecting pathlib2 (from aepp)\n",
      "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tenacity in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aepp) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from pandas->aepp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from pandas->aepp) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from pandas->aepp) (1.23.5)\n",
      "Requirement already satisfied: six in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from pathlib2->aepp) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.3.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from PyJWT[crypto]->aepp) (42.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests->aepp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests->aepp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests->aepp) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests->aepp) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from cryptography>=3.3.1->PyJWT[crypto]->aepp) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3.1->PyJWT[crypto]->aepp) (2.21)\n",
      "Downloading aepp-0.3.4.post2-py3-none-any.whl (169 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.5/169.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pathlib2, aepp\n",
      "Successfully installed aepp-0.3.4.post2 pathlib2-2.3.7.post1\n",
      "Collecting adlfs\n",
      "  Downloading adlfs-2024.4.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from adlfs) (1.26.3)\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46 (from adlfs)\n",
      "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting azure-identity (from adlfs)\n",
      "  Downloading azure_identity-1.16.0-py3-none-any.whl.metadata (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m902.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting azure-storage-blob>=12.12.0 (from adlfs)\n",
      "  Downloading azure_storage_blob-12.20.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: fsspec>=2023.12.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from adlfs) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp>=3.7.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from adlfs) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (4.0.2)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (4.4.0)\n",
      "Requirement already satisfied: cffi in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from azure-datalake-store<0.1,>=0.0.46->adlfs) (1.15.1)\n",
      "Collecting msal<2,>=1.16.0 (from azure-datalake-store<0.1,>=0.0.46->adlfs)\n",
      "  Downloading msal-1.28.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core<2.0.0,>=1.23.1 (from adlfs)\n",
      "  Downloading azure_core-1.30.1-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from azure-storage-blob>=12.12.0->adlfs) (42.0.5)\n",
      "Collecting typing-extensions>=4.0.1 (from azure-core<2.0.0,>=1.23.1->adlfs)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-storage-blob>=12.12.0->adlfs)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting msal-extensions>=0.3.0 (from azure-identity->adlfs)\n",
      "  Downloading msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: pycparser in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from cffi->azure-datalake-store<0.1,>=0.0.46->adlfs) (2.21)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.46->adlfs) (2.4.0)\n",
      "Requirement already satisfied: packaging in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from msal-extensions>=0.3.0->azure-identity->adlfs) (23.0)\n",
      "Collecting portalocker<3,>=1.0 (from msal-extensions>=0.3.0->azure-identity->adlfs)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (2024.2.2)\n",
      "Downloading adlfs-2024.4.1-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m471.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m804.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_storage_blob-12.20.0-py3-none-any.whl (392 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.2/392.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m172.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_identity-1.16.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.1/166.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m618.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msal-1.28.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typing-extensions, portalocker, isodate, azure-core, azure-storage-blob, msal, msal-extensions, azure-datalake-store, azure-identity, adlfs\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: azure-core\n",
      "    Found existing installation: azure-core 1.26.3\n",
      "    Uninstalling azure-core-1.26.3:\n",
      "      Successfully uninstalled azure-core-1.26.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lale 0.7.7 requires click==8.0.4, but you have click 8.1.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed adlfs-2024.4.1 azure-core-1.30.1 azure-datalake-store-0.0.53 azure-identity-1.16.0 azure-storage-blob-12.20.0 isodate-0.6.1 msal-1.28.0 msal-extensions-1.1.0 portalocker-2.8.2 typing-extensions-4.11.0\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.12.3-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from s3fs) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from s3fs) (3.9.3)\n",
      "Collecting botocore<1.34.70,>=1.34.41 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.34.69-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.18)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Downloading s3fs-2024.3.1-py3-none-any.whl (29 kB)\n",
      "Downloading aiobotocore-2.12.3-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading botocore-1.34.69-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.59\n",
      "    Uninstalling botocore-1.27.59:\n",
      "      Successfully uninstalled botocore-1.27.59\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.24.28 requires botocore<1.28.0,>=1.27.28, but you have botocore 1.34.69 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.12.3 aioitertools-0.11.0 botocore-1.34.69 s3fs-2024.3.1\n",
      "Requirement already satisfied: fsspec in /opt/ibm/conda/miniconda3.10/lib/python3.10/site-packages (2024.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n",
    "!pip install aepp\n",
    "!pip install adlfs\n",
    "!pip install s3fs\n",
    "!pip install fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c89d3821bb7346bebbe6c7692f7afe4b"
   },
   "source": [
    "Now lets init the APIClient in order to be able to interact with the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "df19f39e370d4c6492ae315137b09156"
   },
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    \"instance_id\": \"openshift\",\n",
    "    \"version\": \"4.8\",\n",
    "    \"url\": \"https://cpd-cpd-instance.apps.p712zf6h.eastus2.aroapp.io\",\n",
    "    \"username\": watson_username,\n",
    "    \"apikey\": watson_apikey\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bdd232ec9a764f5ba245ce5d20484e86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: 506b7b7a-ecf6-454f-8931-6d1aab37044f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "project_id = project.get_metadata()['metadata']['guid']\n",
    "print(\"Project ID:\", project_id)    \n",
    "\n",
    "client = APIClient(wml_credentials)\n",
    "client.set.default_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9924bf1d40ae47f5b833b62e9cd32155"
   },
   "source": [
    "We'll be using the [aepp Python library](https://github.com/pitchmuc/aepp) here to interact with AEP APIs and create a schema and dataset suitable for adding our synthetic data further down the line. This library simply provides a programmatic interface around the REST APIs, but all these steps could be completed similarly using the raw APIs directly or even in the UI. \n",
    "For more information on the underlying APIs please see the [API reference guide](https://developer.adobe.com/experience-platform-apis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6213d5eb1cdf490c961140d973bd6baa"
   },
   "source": [
    "Before any calls can take place, we need to configure the library and setup authentication credentials. For this you'll need the following piece of information. For information about how you can get these, please refer to the __Setup__ section of the __Readme__:\n",
    "\n",
    "- Client ID\n",
    "- Client secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0ced1e760a74456d834dfb23b1e8a4f8"
   },
   "outputs": [],
   "source": [
    "import aepp\n",
    "\n",
    "aepp.configure(\n",
    "  org_id=ims_org_id,\n",
    "  secret=client_secret,\n",
    "  scopes=scopes,\n",
    "  client_id=client_id,\n",
    "  environment=environment,\n",
    "  sandbox=sandbox_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5abb733188342db8fa8ad7e0708ea83"
   },
   "source": [
    "# 1. Generating Propensity Scores Using the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4cc91e64c164e9481d39112340f1672"
   },
   "source": [
    "### 1.1 Reading the Featurized Data from the Data Landing Zone\n",
    "\n",
    "In the second weekly assignment we had written our featurized data into the Data Landing Zone, and then on the third assignment we just read a sampled\n",
    "portion of it for training our model. At that point we want to score all of the profiles, so we need to read everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e376902c4d5402cbe41234e24103ab2"
   },
   "source": [
    "The featurized data exported into the Data Landing Zone is under the format __cmle/egress__/𝐷𝐴𝑇𝐴𝑆𝐸𝑇𝐼𝐷/𝑒𝑥𝑝𝑜𝑟𝑡𝑇𝑖𝑚𝑒=__EXPORTTIME__.\n",
    "We know the dataset ID which is in your config under `featurized_dataset_id` so we're just missing the export time so we know what to read.\n",
    "To get that we can simply list files in the DLZ and find what the value is. The first step is to retrieve the credentials for the DLZ related to the destination container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "745fbdbb073d4b96bbed2a5992246ca0"
   },
   "outputs": [],
   "source": [
    "import aepp\n",
    "from aepp import flowservice\n",
    "\n",
    "flow_conn = flowservice.FlowService()\n",
    "credentials = flow_conn.getLandingZoneCredential(dlz_type='dlz_destination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "498786f8b2974329b7b2f80bc29f08a3"
   },
   "source": [
    "Now we use some Python libraries to authenticate and issue listing commands so we can get the paths and extract the time from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0fbcca0e861a4629822fa738a129dbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using featurized data export time of 20240506204134\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "from fsspec import AbstractFileSystem\n",
    "\n",
    "def getDLZFSPath(credentials: dict):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        aws_credentials = {\n",
    "            'key' : credentials['credentials']['awsAccessKeyId'],\n",
    "            'secret' : credentials['credentials']['awsSecretAccessKey'],\n",
    "            'token' : credentials['credentials']['awsSessionToken']\n",
    "        }\n",
    "        return fsspec.filesystem('s3', **aws_credentials), credentials['dlzPath']['bucketName']\n",
    "    else:\n",
    "        abs_credentials = {\n",
    "            'account_name' : credentials['storageAccountName'],\n",
    "            'sas_token' : credentials['SASToken']\n",
    "        }\n",
    "        return fsspec.filesystem('abfss', **abs_credentials), credentials['containerName']\n",
    "\n",
    "    \n",
    "def get_export_time(fs: AbstractFileSystem, container_name: str, base_path: str, dataset_id: str):\n",
    "  featurized_data_base_path = f\"{container_name}/{base_path}/{dataset_id}\"\n",
    "  featurized_data_export_paths = fs.ls(featurized_data_base_path)\n",
    "  \n",
    "  if len(featurized_data_export_paths) == 0:\n",
    "    raise Exception(f\"Found no exports for featurized data from dataset ID {dataset_id} under path {featurized_data_base_path}\")\n",
    "  elif len(featurized_data_export_paths) > 1:\n",
    "    print(f\"Found {len(featurized_data_export_paths)} exports from dataset dataset ID {dataset_id} under path {featurized_data_base_path}, using most recent one\")\n",
    "  \n",
    "  featurized_data_export_path = featurized_data_export_paths[-1]\n",
    "  featurized_data_export_time = featurized_data_export_path.strip().split(\"/\")[-1].split(\"=\")[-1]\n",
    "  return featurized_data_export_time\n",
    "\n",
    "fs, container = getDLZFSPath(credentials)\n",
    "\n",
    "export_time = get_export_time(fs, container, export_path, featurized_dataset_id)\n",
    "print(f\"Using featurized data export time of {export_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b243939984c445c8cb311b88673f0fc"
   },
   "source": [
    "At that point we're ready to read this data. We're using Spark since it could be pretty large as we're not doing any sampling. \n",
    "Based on the provisioned account Landing Zone could be either configured to use azure or aws, in case of azure following properties will be used to authenticate using SAS:\n",
    "\n",
    "- `fs.azure.account.auth.type.$ACCOUNT.dfs.core.windows.net` should be set to `SAS`.\n",
    "- `fs.azure.sas.token.provider.type.$ACCOUNT.dfs.core.windows.net` should be set to `org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider`.\n",
    "- `fs.azure.sas.fixed.token.$ACCOUNT.dfs.core.windows.net` should be set to the SAS token retrieved earlier.\n",
    "\n",
    "in case of aws following properties will be used to access data stored in s3:\n",
    "\n",
    "- `fs.s3a.access.key` and spark.hadoop.fs.s3a.access.key` should be the s3 access key\n",
    "- `fs.s3a.secret.key` and spark.hadoop.fs.s3a.secret.key` should be the s3 secret\n",
    "- `fs.s3a.session.token` and `spark.hadoop.fs.s3a.session.token` should be set to s3 session token\n",
    "- `fs.s3a.aws.credentials.provider` and `spark.hadoop.fs.s3a.aws.credentials.provider` should be set to `org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider`\n",
    "- `fs.s3.impl` and `spark.hadoop.fs.s3.impl` should be set to `org.apache.hadoop.fs.s3a.S3AFileSystem`\n",
    "\n",
    "The above properties are calculated based on the landing zone credentials, following util method will set these up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "afe6254d9635422a984a118c6ec59dbc"
   },
   "outputs": [],
   "source": [
    "def configureSparkSessionAndGetPath(credentials):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        aws_key = credentials['credentials']['awsAccessKeyId']\n",
    "        aws_secret = credentials['credentials']['awsSecretAccessKey']\n",
    "        aws_token = credentials['credentials']['awsSessionToken']\n",
    "        aws_buket = credentials['dlzPath']['bucketName']\n",
    "        dlz_folder = credentials['dlzPath']['dlzFolder']\n",
    "        spark.conf.set(\"fs.s3a.access.key\", aws_key)\n",
    "        spark.conf.set(\"fs.s3a.secret.key\", aws_secret)\n",
    "        spark.conf.set(\"fs.s3a.session.token\", aws_token)\n",
    "        spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "        spark.conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_key)\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_secret)\n",
    "        spark.conf.set(\"fs.s3a.session.token\", aws_token)\n",
    "        return f\"s3a://{aws_buket}/{dlz_folder}/\"\n",
    "    else:\n",
    "        dlz_storage_account = credentials['storageAccountName']\n",
    "        dlz_sas_token = credentials['SASToken']\n",
    "        dlz_container = credentials['containerName']    \n",
    "        spark.conf.set(f\"fs.azure.account.auth.type.{dlz_storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "        spark.conf.set(f\"fs.azure.sas.token.provider.type.{dlz_storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "        spark.conf.set(f\"fs.azure.sas.fixed.token.{dlz_storage_account}.dfs.core.windows.net\", dlz_sas_token)\n",
    "        return f\"abfss://{dlz_container}@{dlz_storage_account}.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7a6d76725f04a3f8244f91a066d1ff1"
   },
   "source": [
    "Let's put that in practice and create a Spark dataframe containing the entire featurized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9ba8b1ee88714e229ae5808152040b3a"
   },
   "outputs": [],
   "source": [
    "cloud_base_path = configureSparkSessionAndGetPath(credentials)\n",
    "input_path = cloud_base_path + f\"{export_path}/{featurized_dataset_id}/exportTime={export_time}/\"\n",
    "\n",
    "df = spark.read.parquet(input_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "274f36af91cf4bf6b0a52e86c1819d83"
   },
   "source": [
    "We can verify it matches what we had written out in the second weekly assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "d3dd1f32fe3649ddbf9ef587e85ea6e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399624"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d727df3384874fe680495b729899cbdc"
   },
   "source": [
    "And also do a sanity check on the data to make sure it looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "090820b23001447a83dcbd9eb57ec83c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "|              userId|           eventType|           timestamp|subscriptionOccurred|emailsReceived|emailsOpened|emailsClicked|productsViewed|propositionInteracts|propositionDismissed|webLinkClicks|minutes_since_emailSent|minutes_since_emailOpened|minutes_since_emailClick|minutes_since_productView|minutes_since_propositionInteract|minutes_since_propositionDismiss|minutes_since_linkClick|random_row_number_for_user|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "|28897100435685448...|directMarketing.e...|2024-03-03 13:52:...|                   0|             4|           2|            0|             0|                   0|                   0|            1|                   1363|                        0|                    null|                     null|                             null|                            null|                  11159|                         1|\n",
      "|24307197848604435...|directMarketing.e...|2024-03-03 13:54:...|                   0|             3|           3|            1|             0|                   0|                   0|            0|                   1795|                       64|                       0|                     null|                             null|                            null|                   null|                         1|\n",
      "|68384671444663356...|directMarketing.e...|2024-03-03 13:54:...|                   0|             9|           3|            1|             0|                   0|                   0|            0|                      0|                    72565|                   72564|                     null|                             null|                            null|                   null|                         1|\n",
      "|01344397048615061...|   web.formFilledOut|2024-03-03 13:54:...|                   1|             1|           2|            1|             1|                   0|                   0|            1|                   2303|                      147|                      20|                        1|                             null|                            null|                      1|                         4|\n",
      "|42016943133706250...|directMarketing.e...|2024-03-03 13:55:...|                   0|             3|           3|            0|             0|                   0|                   0|            0|                      0|                    34688|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|06133951984998603...|directMarketing.e...|2024-03-03 13:56:...|                   0|             1|           2|            2|             0|                   0|                   0|            0|                   3136|                       26|                       0|                     null|                             null|                            null|                   null|                         1|\n",
      "|46175594460361143...|directMarketing.e...|2024-03-03 13:56:...|                   0|             4|           0|            0|             0|                   0|                   0|            0|                      0|                     null|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|46458824736215741...|directMarketing.e...|2024-03-03 13:56:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                     null|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|49146231207393700...|directMarketing.e...|2024-03-03 13:57:...|                   0|             9|           3|            0|             0|                   0|                   0|            0|                      0|                    32760|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|71133680023352327...|directMarketing.e...|2024-03-03 13:57:...|                   0|             4|           1|            1|             0|                   0|                   0|            1|                      0|                    41664|                   41615|                     null|                             null|                            null|                  41603|                         1|\n",
      "|21392370132348367...|web.webpagedetail...|2024-03-03 13:58:...|                   0|             5|           3|            0|             0|                   0|                   0|            0|                  11352|                    31600|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|43127820680721913...|directMarketing.e...|2024-03-03 13:58:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                     null|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|40652970437400650...|  commerce.purchases|2024-03-03 13:59:...|                   0|             6|           1|            0|             1|                   0|                   0|            0|                   8010|                    32910|                    null|                       72|                             null|                            null|                   null|                         1|\n",
      "|70315090259130256...|directMarketing.e...|2024-03-03 13:59:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                     null|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|86096732042298572...|directMarketing.e...|2024-03-03 14:03:...|                   0|             4|           3|            0|             0|                   0|                   0|            0|                      0|                    21059|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|29266217093095880...|directMarketing.e...|2024-03-03 14:04:...|                   0|             5|           1|            0|             0|                   0|                   0|            0|                    598|                        0|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|64793272043053819...|directMarketing.e...|2024-03-03 14:05:...|                   0|             4|           1|            0|             0|                   0|                   0|            0|                    989|                        0|                    null|                     null|                             null|                            null|                   null|                         1|\n",
      "|66372548702699006...|directMarketing.e...|2024-03-03 14:07:...|                   0|             8|           1|            0|             0|                   0|                   0|            1|                      0|                    49203|                    null|                     null|                             null|                            null|                   8702|                         1|\n",
      "|78286194958254499...|directMarketing.e...|2024-03-03 14:07:...|                   0|             6|           1|            1|             0|                   0|                   0|            0|                      0|                    55746|                   55744|                     null|                             null|                            null|                   null|                         1|\n",
      "|34031057116835446...|   web.formFilledOut|2024-03-03 14:07:...|                   1|             4|           1|            1|             0|                   0|                   0|            0|                   1417|                       30|                       6|                     null|                             null|                            null|                   null|                         9|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "938002a4efe846f881f2acd222bf086e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "|              userId|           eventType|           timestamp|subscriptionOccurred|emailsReceived|emailsOpened|emailsClicked|productsViewed|propositionInteracts|propositionDismissed|webLinkClicks|minutes_since_emailSent|minutes_since_emailOpened|minutes_since_emailClick|minutes_since_productView|minutes_since_propositionInteract|minutes_since_propositionDismiss|minutes_since_linkClick|random_row_number_for_user|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "|28897100435685448...|directMarketing.e...|2024-03-03 13:52:...|                   0|             4|           2|            0|             0|                   0|                   0|            1|                   1363|                        0|                       0|                        0|                                0|                               0|                  11159|                         1|\n",
      "|24307197848604435...|directMarketing.e...|2024-03-03 13:54:...|                   0|             3|           3|            1|             0|                   0|                   0|            0|                   1795|                       64|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|68384671444663356...|directMarketing.e...|2024-03-03 13:54:...|                   0|             9|           3|            1|             0|                   0|                   0|            0|                      0|                    72565|                   72564|                        0|                                0|                               0|                      0|                         1|\n",
      "|01344397048615061...|   web.formFilledOut|2024-03-03 13:54:...|                   1|             1|           2|            1|             1|                   0|                   0|            1|                   2303|                      147|                      20|                        1|                                0|                               0|                      1|                         4|\n",
      "|42016943133706250...|directMarketing.e...|2024-03-03 13:55:...|                   0|             3|           3|            0|             0|                   0|                   0|            0|                      0|                    34688|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|06133951984998603...|directMarketing.e...|2024-03-03 13:56:...|                   0|             1|           2|            2|             0|                   0|                   0|            0|                   3136|                       26|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|46175594460361143...|directMarketing.e...|2024-03-03 13:56:...|                   0|             4|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|46458824736215741...|directMarketing.e...|2024-03-03 13:56:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|49146231207393700...|directMarketing.e...|2024-03-03 13:57:...|                   0|             9|           3|            0|             0|                   0|                   0|            0|                      0|                    32760|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|71133680023352327...|directMarketing.e...|2024-03-03 13:57:...|                   0|             4|           1|            1|             0|                   0|                   0|            1|                      0|                    41664|                   41615|                        0|                                0|                               0|                  41603|                         1|\n",
      "|21392370132348367...|web.webpagedetail...|2024-03-03 13:58:...|                   0|             5|           3|            0|             0|                   0|                   0|            0|                  11352|                    31600|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|43127820680721913...|directMarketing.e...|2024-03-03 13:58:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|40652970437400650...|  commerce.purchases|2024-03-03 13:59:...|                   0|             6|           1|            0|             1|                   0|                   0|            0|                   8010|                    32910|                       0|                       72|                                0|                               0|                      0|                         1|\n",
      "|70315090259130256...|directMarketing.e...|2024-03-03 13:59:...|                   0|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|86096732042298572...|directMarketing.e...|2024-03-03 14:03:...|                   0|             4|           3|            0|             0|                   0|                   0|            0|                      0|                    21059|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|29266217093095880...|directMarketing.e...|2024-03-03 14:04:...|                   0|             5|           1|            0|             0|                   0|                   0|            0|                    598|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|64793272043053819...|directMarketing.e...|2024-03-03 14:05:...|                   0|             4|           1|            0|             0|                   0|                   0|            0|                    989|                        0|                       0|                        0|                                0|                               0|                      0|                         1|\n",
      "|66372548702699006...|directMarketing.e...|2024-03-03 14:07:...|                   0|             8|           1|            0|             0|                   0|                   0|            1|                      0|                    49203|                       0|                        0|                                0|                               0|                   8702|                         1|\n",
      "|78286194958254499...|directMarketing.e...|2024-03-03 14:07:...|                   0|             6|           1|            1|             0|                   0|                   0|            0|                      0|                    55746|                   55744|                        0|                                0|                               0|                      0|                         1|\n",
      "|34031057116835446...|   web.formFilledOut|2024-03-03 14:07:...|                   1|             4|           1|            1|             0|                   0|                   0|            0|                   1417|                       30|                       6|                        0|                                0|                               0|                      0|                         9|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(0)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9652b74a9bb464b880f10768f1f3bbe"
   },
   "source": [
    "# 1.2 Scoring the Profiles\n",
    "\n",
    "For scoring we need 2 things:\n",
    "\n",
    "- The __data__ to score\n",
    "- The __trained model__ that will be used to do the scoring\n",
    "\n",
    "We just created a dataframe containing the first one, and in the previous weekly assignment we created a production model that can operate on this data, so let's fetch this model from our repository and turn it into a Spark UDF so it can interact with our data easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8ed034892ee54adca283c10d955d8688"
   },
   "outputs": [],
   "source": [
    "model = client.repository.load(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sklearn_pipeline](images/sklearn_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebe45c4895e440ce84e5981c2ee037b8"
   },
   "source": [
    "Now we're ready to apply our trained model on top of the entire dataset. For that, we need to create an UDF function which will use the model above for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "f3af7dc554ba421db331b6f73060d87f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def model_predict(features):\n",
    "    return float(model.predict_proba(np.array([features]))[:, 1]) # index 1 - represents positive probability of event to occur, while 0 - is probability of negative outcome\n",
    "\n",
    "predict_udf = udf(model_predict, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea8741cb416e4b2cb70df4bc8f161ae3"
   },
   "source": [
    "Now lets define UDF its inputs - which in our case are all the columns that the model needs to to operate on (except the `subscriptionOccurred` - which our model is not expecting but rather is attempting to predict the score for).\n",
    "We can get that easily as the Spark dataframe contains metadata about its columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "221a9e7377494692b3c47affdb4df7b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- eventType: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- emailsReceived: long (nullable = true)\n",
      " |-- emailsOpened: long (nullable = true)\n",
      " |-- emailsClicked: long (nullable = true)\n",
      " |-- productsViewed: long (nullable = true)\n",
      " |-- propositionInteracts: long (nullable = true)\n",
      " |-- propositionDismissed: long (nullable = true)\n",
      " |-- webLinkClicks: long (nullable = true)\n",
      " |-- minutes_since_emailSent: integer (nullable = true)\n",
      " |-- minutes_since_emailOpened: integer (nullable = true)\n",
      " |-- minutes_since_emailClick: integer (nullable = true)\n",
      " |-- minutes_since_productView: integer (nullable = true)\n",
      " |-- minutes_since_propositionInteract: integer (nullable = true)\n",
      " |-- minutes_since_propositionDismiss: integer (nullable = true)\n",
      " |-- minutes_since_linkClick: integer (nullable = true)\n",
      " |-- random_row_number_for_user: integer (nullable = true)\n",
      " |-- prediction: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_to_score = df.drop('subscriptionOccurred') # drop label column since model is not expecting it for scoring purposes\n",
    "\n",
    "udf_inputs = struct(*(df_to_score.columns))\n",
    "\n",
    "df_scored = df_to_score.withColumn(\n",
    "  \"prediction\",\n",
    "  predict_udf(udf_inputs)\n",
    ")\n",
    "\n",
    "df_scored.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "478dfd1e0dd04bed8e7d06440730d107"
   },
   "source": [
    "If we look at the data we should see a new column called prediction which corresponds to the score generated by the model for this particular profile based on all the features computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fa6eb3a2618e423a8f727d703b30a019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+------------+\n",
      "|              userId|           eventType|           timestamp|emailsReceived|emailsOpened|emailsClicked|productsViewed|propositionInteracts|propositionDismissed|webLinkClicks|minutes_since_emailSent|minutes_since_emailOpened|minutes_since_emailClick|minutes_since_productView|minutes_since_propositionInteract|minutes_since_propositionDismiss|minutes_since_linkClick|random_row_number_for_user|  prediction|\n",
      "+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+------------+\n",
      "|28897100435685448...|directMarketing.e...|2024-03-03 13:52:...|             4|           2|            0|             0|                   0|                   0|            1|                   1363|                        0|                       0|                        0|                                0|                               0|                  11159|                         1| 1.363146E-4|\n",
      "|24307197848604435...|directMarketing.e...|2024-03-03 13:54:...|             3|           3|            1|             0|                   0|                   0|            0|                   1795|                       64|                       0|                        0|                                0|                               0|                      0|                         1| 0.005135295|\n",
      "|68384671444663356...|directMarketing.e...|2024-03-03 13:54:...|             9|           3|            1|             0|                   0|                   0|            0|                      0|                    72565|                   72564|                        0|                                0|                               0|                      0|                         1|  0.18483353|\n",
      "|01344397048615061...|   web.formFilledOut|2024-03-03 13:54:...|             1|           2|            1|             1|                   0|                   0|            1|                   2303|                      147|                      20|                        1|                                0|                               0|                      1|                         4|  0.99730885|\n",
      "|42016943133706250...|directMarketing.e...|2024-03-03 13:55:...|             3|           3|            0|             0|                   0|                   0|            0|                      0|                    34688|                       0|                        0|                                0|                               0|                      0|                         1|   0.6605504|\n",
      "|06133951984998603...|directMarketing.e...|2024-03-03 13:56:...|             1|           2|            2|             0|                   0|                   0|            0|                   3136|                       26|                       0|                        0|                                0|                               0|                      0|                         1|  0.44835493|\n",
      "|46175594460361143...|directMarketing.e...|2024-03-03 13:56:...|             4|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1|  0.01690403|\n",
      "|46458824736215741...|directMarketing.e...|2024-03-03 13:56:...|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1| 0.020547912|\n",
      "|49146231207393700...|directMarketing.e...|2024-03-03 13:57:...|             9|           3|            0|             0|                   0|                   0|            0|                      0|                    32760|                       0|                        0|                                0|                               0|                      0|                         1|   0.7941457|\n",
      "|71133680023352327...|directMarketing.e...|2024-03-03 13:57:...|             4|           1|            1|             0|                   0|                   0|            1|                      0|                    41664|                   41615|                        0|                                0|                               0|                  41603|                         1|2.2124968E-4|\n",
      "|21392370132348367...|web.webpagedetail...|2024-03-03 13:58:...|             5|           3|            0|             0|                   0|                   0|            0|                  11352|                    31600|                       0|                        0|                                0|                               0|                      0|                         1| 0.023310047|\n",
      "|43127820680721913...|directMarketing.e...|2024-03-03 13:58:...|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1| 0.020547912|\n",
      "|40652970437400650...|  commerce.purchases|2024-03-03 13:59:...|             6|           1|            0|             1|                   0|                   0|            0|                   8010|                    32910|                       0|                       72|                                0|                               0|                      0|                         1|  0.00211263|\n",
      "|70315090259130256...|directMarketing.e...|2024-03-03 13:59:...|             6|           0|            0|             0|                   0|                   0|            0|                      0|                        0|                       0|                        0|                                0|                               0|                      0|                         1| 0.020547912|\n",
      "|86096732042298572...|directMarketing.e...|2024-03-03 14:03:...|             4|           3|            0|             0|                   0|                   0|            0|                      0|                    21059|                       0|                        0|                                0|                               0|                      0|                         1|   0.4363622|\n",
      "|29266217093095880...|directMarketing.e...|2024-03-03 14:04:...|             5|           1|            0|             0|                   0|                   0|            0|                    598|                        0|                       0|                        0|                                0|                               0|                      0|                         1|0.0031176307|\n",
      "|64793272043053819...|directMarketing.e...|2024-03-03 14:05:...|             4|           1|            0|             0|                   0|                   0|            0|                    989|                        0|                       0|                        0|                                0|                               0|                      0|                         1| 0.002776257|\n",
      "|66372548702699006...|directMarketing.e...|2024-03-03 14:07:...|             8|           1|            0|             0|                   0|                   0|            1|                      0|                    49203|                       0|                        0|                                0|                               0|                   8702|                         1|0.0075649805|\n",
      "|78286194958254499...|directMarketing.e...|2024-03-03 14:07:...|             6|           1|            1|             0|                   0|                   0|            0|                      0|                    55746|                   55744|                        0|                                0|                               0|                      0|                         1|  0.13561456|\n",
      "|34031057116835446...|   web.formFilledOut|2024-03-03 14:07:...|             4|           1|            1|             0|                   0|                   0|            0|                   1417|                       30|                       6|                        0|                                0|                               0|                      0|                         9|  0.67165685|\n",
      "+--------------------+--------------------+--------------------+--------------+------------+-------------+--------------+--------------------+--------------------+-------------+-----------------------+-------------------------+------------------------+-------------------------+---------------------------------+--------------------------------+-----------------------+--------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scored.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "88301edeb73647f4a84844736137cabf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399624 - rows in dataset\n",
      "100000 - unique users in dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_scored.count()} - rows in dataset\")\n",
    "unique_users = df_scored.select('userId').distinct().count()\n",
    "print(f\"{unique_users} - unique users in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a336d94fcf0a4b0c83254ea45bd6939d"
   },
   "source": [
    "Lets now group by userId and pull `MAX` prediction this way discarding duplicated records.\n",
    "The result we should obtain here as many rows as many unique userId's we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5848e3a4f29b4a3481d9c7b3f305d4bf"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_user_predictions = df_scored.groupBy(\"userId\").agg(F.max(\"prediction\").alias(\"max_prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3e636c033144f678b5e4db04cba4ec3"
   },
   "source": [
    "When you think about bringing the scored profiles back into the Adobe Experience Platform, we don't need to bring back all the features. In fact, we only really need 2 columns:\n",
    "\n",
    "    - The user ID, so we know in the Unified Profile to which profile this row corresponds.\n",
    "    - The score for this user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "07401b68592f4dcdb7059d0ea79a732d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- max_prediction: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_to_ingest = df_user_predictions.select(\n",
    "  \"userId\",\n",
    "  \"max_prediction\"\n",
    ").cache()\n",
    "\n",
    "df_to_ingest.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37e7d4c0a7a4486ab62068acfcdc95b9"
   },
   "source": [
    "At that point we have the scored profiles and exactly what we need to bring back into Adobe Experience Platform. But we're not quite ready to write the results yet, there's a bit of setup that needs to happen first:\n",
    "\n",
    "    We need to create and configure a destination dataset in Adobe Experience Platform where our data will end up.\n",
    "    We need to setup a data flow that will be able to take this data, convert it into an XDM format, and deliver it to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6aa5a4eedeb4fcfa012d40a5752996c"
   },
   "source": [
    "# 2. Bringing the Scores back into Unified Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13b9c9db7078426391b012d34180dff5"
   },
   "source": [
    "### 2.1 Create ingestion schema and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24fc2dda5f5c465c9a740baec815994b"
   },
   "source": [
    "The first step is to define where this propensity data we are creating as the output of our model should end up in the Unified Profile. We need to create a few entities for that:\n",
    "\n",
    "    A **fieldgroup** that will define the XDM for where propensity scores should be stored.\n",
    "    A **schema** based on that field group that will tie it back to the concept of profile.\n",
    "    A **dataset** based on that schema that will hold the data.\n",
    "\n",
    "As for the structure itself it's pretty simple, we just need 2 fields:\n",
    "\n",
    "    The **propensity** itself as a decimal number.\n",
    "    The **user ID** to which this propensity score relates.\n",
    "\n",
    "Let's put that in practice and create the field group. Note that because we are creating custom fields here, they need to be nested under the tenant ID corresponding to your organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ae382282f59e4b3d88183be0b7aa5019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloudmlecosystem'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aepp import schema\n",
    "\n",
    "schema_conn = schema.Schema()\n",
    "\n",
    "tenant_id = schema_conn.getTenantId()\n",
    "tenant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "62585fbd408c43969c0f7ce48428a315"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ns.adobe.com/cloudmlecosystem/mixins/c4ef85f3bdd05fb971b8603596ebb000113f97a7e34485f9'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldgroup_res = schema_conn.createFieldGroup({\n",
    "  \t\"type\": \"object\",\n",
    "\t\"title\": f\"[CMLE][Week4] Fieldgroup for user propensity (created by {username})\",\n",
    "\t\"description\": \"This mixin is used to define a propensity score that can be assigned to a given profile.\",\n",
    "\t\"allOf\": [{\n",
    "\t\t\"$ref\": \"#/definitions/customFields\"\n",
    "\t}],\n",
    "\t\"meta:containerId\": \"tenant\",\n",
    "\t\"meta:resourceType\": \"mixins\",\n",
    "\t\"meta:xdmType\": \"object\",\n",
    "\t\"definitions\": {\n",
    "      \"customFields\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          f\"_{tenant_id}\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"propensity\": {\n",
    "                \"title\": \"Propensity\",\n",
    "                \"description\": \"This refers to the propensity of a user towards an outcome.\",\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"userid\": {\n",
    "                \"title\": \"User ID\",\n",
    "                \"description\": \"This refers to the user having a propensity towards an outcome.\",\n",
    "                \"type\": \"string\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "\t},\n",
    "\t\"meta:intendedToExtend\": [\"https://ns.adobe.com/xdm/context/profile\"]\n",
    "})\n",
    "\n",
    "fieldgroup_id = fieldgroup_res[\"$id\"]\n",
    "fieldgroup_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00fe728fa88249ef84cc82946cc6b0f6"
   },
   "source": [
    "From this field group ID we can add it to a brand new schema that will be marked for profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "25bd0593167344cc829b77f2d09ec6d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema ID: https://ns.adobe.com/cloudmlecosystem/schemas/cd4be6112bc779d88f46cf0539a296f0207de2caf219f02f\n",
      "Schema Alt ID: _cloudmlecosystem.schemas.cd4be6112bc779d88f46cf0539a296f0207de2caf219f02f\n"
     ]
    }
   ],
   "source": [
    "schema_res = schema_conn.createProfileSchema(\n",
    "  name=f\"[CMLE][Week4] Schema for user propensity ingestion (created by {username})\",\n",
    "  mixinIds=[\n",
    "    fieldgroup_id\n",
    "  ],\n",
    "  description=\"Schema generated by CMLE for user propensity score ingestion\",\n",
    ")\n",
    "\n",
    "schema_id = schema_res[\"$id\"]\n",
    "schema_alt_id = schema_res[\"meta:altId\"]\n",
    "\n",
    "print(f\"Schema ID: {schema_id}\")\n",
    "print(f\"Schema Alt ID: {schema_alt_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1566446181f47a5aae9f5b2fff1af4d"
   },
   "source": [
    "Because we eventually intend for these scores to end up in the Unified Profile, we need to specify which field of the schema corresponds to an identity so it can resolve the corresponding profile. In our case, the `userid` field is an ECID and we mark it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a7845f30b18942c981ae5eb9d10331b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@id': 'abb9cf5e95479ef68bb095515d4607fb5ba4c2fe0c6d404b',\n",
       " '@type': 'xdm:descriptorIdentity',\n",
       " 'xdm:sourceSchema': 'https://ns.adobe.com/cloudmlecosystem/schemas/cd4be6112bc779d88f46cf0539a296f0207de2caf219f02f',\n",
       " 'xdm:sourceVersion': 1,\n",
       " 'xdm:sourceProperty': '/_cloudmlecosystem/userid',\n",
       " 'imsOrg': '3ADF23C463D98F640A494032@AdobeOrg',\n",
       " 'version': '1',\n",
       " 'xdm:namespace': 'ECID',\n",
       " 'xdm:property': 'xdm:id',\n",
       " 'xdm:isPrimary': True,\n",
       " 'meta:containerId': '21f60eb6-7c13-4074-b60e-b67c13b0740c',\n",
       " 'meta:sandboxId': '21f60eb6-7c13-4074-b60e-b67c13b0740c',\n",
       " 'meta:sandboxType': 'production'}"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_type = \"ECID\"\n",
    "descriptor_res = schema_conn.createDescriptor(\n",
    "  descriptorObj = {\n",
    "    \"@type\": \"xdm:descriptorIdentity\",\n",
    "    \"xdm:sourceSchema\": schema_id,\n",
    "    \"xdm:sourceVersion\": 1,\n",
    "    \"xdm:sourceProperty\": f\"/_{tenant_id}/userid\",\n",
    "    \"xdm:namespace\": identity_type,\n",
    "    \"xdm:property\": \"xdm:id\",\n",
    "    \"xdm:isPrimary\": True\n",
    "  }\n",
    ")\n",
    "descriptor_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38d65655f99547db9e5970b8fc587ea1"
   },
   "source": [
    "And of course that schema needs to be enabled for Unified Profile consumption, so it can be added to the profile union schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7d6e2c64817942098e5a3c90a721ca78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$id': 'https://ns.adobe.com/cloudmlecosystem/schemas/cd4be6112bc779d88f46cf0539a296f0207de2caf219f02f',\n",
       " 'meta:altId': '_cloudmlecosystem.schemas.cd4be6112bc779d88f46cf0539a296f0207de2caf219f02f',\n",
       " 'meta:resourceType': 'schemas',\n",
       " 'version': '1.1',\n",
       " 'title': '[CMLE][Week4] Schema for user propensity ingestion (created by mndymuqvx34peqwqz-ydi68gcdn1kj9ugzqs-towtum)',\n",
       " 'type': 'object',\n",
       " 'description': 'Schema generated by CMLE for user propensity score ingestion',\n",
       " 'allOf': [{'$ref': 'https://ns.adobe.com/xdm/context/profile',\n",
       "   'type': 'object',\n",
       "   'meta:xdmType': 'object'},\n",
       "  {'$ref': 'https://ns.adobe.com/cloudmlecosystem/mixins/c4ef85f3bdd05fb971b8603596ebb000113f97a7e34485f9',\n",
       "   'type': 'object',\n",
       "   'meta:xdmType': 'object'}],\n",
       " 'refs': ['https://ns.adobe.com/xdm/context/profile',\n",
       "  'https://ns.adobe.com/cloudmlecosystem/mixins/c4ef85f3bdd05fb971b8603596ebb000113f97a7e34485f9'],\n",
       " 'imsOrg': '3ADF23C463D98F640A494032@AdobeOrg',\n",
       " 'additionalInfo': {'numberOfIdentities': 1,\n",
       "  'numberOfRelationShips': 0,\n",
       "  'classTitle': 'XDM Individual Profile',\n",
       "  'behavior': 'Record',\n",
       "  'hasPrimaryIdentity': 'true',\n",
       "  'primaryIdentityNamespace': 'ECID',\n",
       "  'hasRelationShip': False,\n",
       "  'profileEnabled': True},\n",
       " 'meta:extensible': False,\n",
       " 'meta:abstract': False,\n",
       " 'meta:extends': ['https://ns.adobe.com/xdm/common/auditable',\n",
       "  'https://ns.adobe.com/xdm/data/record',\n",
       "  'https://ns.adobe.com/xdm/context/profile',\n",
       "  'https://ns.adobe.com/cloudmlecosystem/mixins/c4ef85f3bdd05fb971b8603596ebb000113f97a7e34485f9'],\n",
       " 'meta:xdmType': 'object',\n",
       " 'meta:registryMetadata': {'repo:createdDate': 1715188594366,\n",
       "  'repo:lastModifiedDate': 1715188665781,\n",
       "  'xdm:createdClientId': 'b82c15b5950b4c3a87d6303c58692c94',\n",
       "  'xdm:lastModifiedClientId': 'b82c15b5950b4c3a87d6303c58692c94',\n",
       "  'xdm:createdUserId': '42F21963661559FA0A494129@techacct.adobe.com',\n",
       "  'xdm:lastModifiedUserId': '42F21963661559FA0A494129@techacct.adobe.com',\n",
       "  'eTag': 'b08385b98761ef6f51eba3bd438dd1bf7145d84ba3d60eea43c8b48b3fb31119',\n",
       "  'meta:globalLibVersion': '1.49.2'},\n",
       " 'meta:class': 'https://ns.adobe.com/xdm/context/profile',\n",
       " 'meta:containerId': '21f60eb6-7c13-4074-b60e-b67c13b0740c',\n",
       " 'meta:sandboxId': '21f60eb6-7c13-4074-b60e-b67c13b0740c',\n",
       " 'meta:sandboxType': 'production',\n",
       " 'meta:tenantNamespace': '_cloudmlecosystem',\n",
       " 'meta:immutableTags': ['union'],\n",
       " 'meta:descriptorStatus': {'result': []}}"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_res = schema_conn.enableSchemaForRealTime(schema_alt_id)\n",
    "enable_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c28ea1a7b2dd4b4393a1770e825f21ba"
   },
   "source": [
    "At that point we're ready to create the dataset that will hold our propensity scores. This dataset is based on our schema we just created and nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fe4305ee355d4ba38f2ae86c3d8330bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'663bb3d786def12c699779a1'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aepp import catalog\n",
    "\n",
    "cat_conn = catalog.Catalog()\n",
    "\n",
    "ingestion_dataset_res = cat_conn.createDataSets(\n",
    "  name=f\"[CMLE][Week4] Dataset for user propensity ingestion (created by {username})\",\n",
    "  schemaId=schema_id\n",
    ")\n",
    "\n",
    "ingestion_dataset_id = ingestion_dataset_res[0].split(\"/\")[-1]\n",
    "ingestion_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95c869575d8a4dab80e5b5795b6b4bbe"
   },
   "source": [
    "And similarly that dataset needs to be enabled for Unified Profile consumption, so that any batch of data written to this dataset is automatically picked up and processed to insert into the individual profiles and create new fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "14494ffd3a8a43398cca95ef26d0d29d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@/dataSets/663bb3d786def12c699779a1']"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: this is currently failing due to invalid content type, need to fix in aepp, see https://github.com/pitchmuc/aepp/issues/15\n",
    "# for now just enable in the UI...\n",
    "cat_conn.enableDatasetProfile(ingestion_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf7a08d4cf364ae3822936b735f6a0e6"
   },
   "source": [
    "You should be able to see your dataset in the UI at the link below, and it should match the new schema created as shown in the following screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ce1f2069ce2a44b2a62cbbb55b835a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID 663bb3d786def12c699779a1 available under https://experience-stage.adobe.com/#/@cloudmlecosystem/sname:cmle-datarobot/platform/dataset/browse/663bb3d786def12c699779a1\n"
     ]
    }
   ],
   "source": [
    "ingestion_dataset_link = get_ui_link(tenant_id, \"dataset/browse\", ingestion_dataset_id)\n",
    "print(f\"Dataset ID {ingestion_dataset_id} available under {ingestion_dataset_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0796f18dff524931b73176b26b4d12f3"
   },
   "source": [
    "## 2.2 Setup ingestion data flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6275d0a53724df881191df4014b9701"
   },
   "source": [
    "Now that all the dataset and schema setup is completed, we're ready to define our Data Flow. The Data Flow defines the contract between the source and destination dataset.\n",
    "\n",
    "For the purposes of this notebook we will be using the [Data Landing Zone (DLZ)](https://experienceleague.adobe.com/docs/experience-platform/sources/api-tutorials/create/cloud-storage/data-landing-zone.html?lang=en) as the source filesystem under which the scoring results will be written. Every Adobe Experience Platform has a DLZ already setup as an [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs) container or [AWS S3](https://aws.amazon.com/s3/). We'll be using that as a delivery mechanism for the featurized data, but this step can be customized to delivery this data to any cloud storage filesystem.\n",
    "To setup the delivery pipeline, we'll be using the Flow Service for Destinations which will be responsible for picking up the featurized data and dump it into the DLZ. There's a few steps involved:\n",
    "    \n",
    "Creating a source connection.\n",
    "Creating a target connection.\n",
    "Creating a transformation.\n",
    "Creating a data flow.\n",
    "\n",
    "Note that, although we already got DLZ credentials earlier in this notebook, there were for a different container where all the destination data is written (`dlz-destination`), but here we want to get credentials for a different container corresponding to your user drop zone (`dlz-user-container`).\n",
    "\n",
    "For that, again we use aepp to abstract all the APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "e13285efb262460e969bcf3a34d69bda"
   },
   "outputs": [],
   "source": [
    "from aepp import flowservice\n",
    "\n",
    "flow_conn = flowservice.FlowService()\n",
    "dlz_credentials = flow_conn.getLandingZoneCredential()\n",
    "\n",
    "def getDLZPath(credentials: dict):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        return credentials['dlzPath']['bucketName'] + '/' + credentials['dlzPath']['dlzFolder']\n",
    "    else:\n",
    "        return credentials['containerName']\n",
    "\n",
    "dlz_container = getDLZPath(dlz_credentials)\n",
    "print(dlz_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8265769e2bf841398c813637c0f29a9d"
   },
   "source": [
    "The __source connection__ is responsible for connecting to your cloud storage account (in our case here, the Data Landing Zone) so that the resulting Data Flow will know from where data needs to be picked up.\n",
    "\n",
    "For reference, here is a list of all the connection specs available for the most popular cloud storage accounts (these IDs are global across every single customer account and sandbox):\n",
    "    \n",
    "| Cloud Storage Type | Connection Spec ID | Connection Spec Name \n",
    "|--------------------|--------------------------------------|---------------------- \n",
    "| Amazon S3          | ecadc60c-7455-4d87-84dc-2a0e293d997b | amazon-s3 \n",
    "| Azure Blob Storage | d771e9c1-4f26-40dc-8617-ce58c4b53702 | google-adwords \n",
    "| Azure Data Lake    | b3ba5556-48be-44b7-8b85-ff2b69b46dc4 | adls-gen2 \n",
    "| Data Landing Zone  | 26f526f2-58f4-4712-961d-e41bf1ccc0e8 | landing-zone \n",
    "| Google Cloud Storage| 32e8f412-cdf7-464c-9885-78184cb113fd | google-cloud \n",
    "| SFTP               | b7bf2577-4520-42c9-bae9-cad01560f7bc | sftp    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "db077dfc3df74adba3915d38a09f52ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc8479a9-c04f-4672-9729-1af5b1bedced'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_spec_id = \"26f526f2-58f4-4712-961d-e41bf1ccc0e8\"\n",
    "source_res = flow_conn.createSourceConnection({\n",
    "  \"name\": \"[CMLE][Week4] Data Landing Zone source connection for propensity scores\",\n",
    "  \"data\": {\n",
    "      \"format\": \"delimited\"\n",
    "  },\n",
    "  \"params\": {\n",
    "    \"path\": f\"{dlz_container}/{import_path}\",\n",
    "    \"type\": \"folder\",\n",
    "    \"recursive\": True\n",
    "  },\n",
    "  \"connectionSpec\": {\n",
    "      \"id\": connection_spec_id,\n",
    "      \"version\": \"1.0\"\n",
    "  }\n",
    "})\n",
    "\n",
    "source_connection_id = source_res[\"id\"]\n",
    "source_connection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1427234efabf49728ee289febe0a6aaf"
   },
   "source": [
    "The __target connection__ is responsible for connecting to your Adobe Experience Platform dataset so that the resulting Data Flow will know where the data needs to be written. Because we already created our ingestion dataset in the previous section, we can simply tie it to that dataset ID and the corresponding schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "83ef5fd9d8014e7f85bfd3a072db3571"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e45cfb70-ea1e-4301-bcfa-20b81c00362b'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_res = flow_conn.createTargetConnectionDataLake(\n",
    "  name=\"[CMLE][Week4] User Propensity Target Connection\",\n",
    "  datasetId=ingestion_dataset_id,\n",
    "  schemaId=schema_id\n",
    ")\n",
    "\n",
    "target_connection_id = target_res[\"id\"]\n",
    "target_connection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "032ac61cdb4740458d8077520487bf73"
   },
   "source": [
    "We're still missing one step. If you look back to the previous cells, this is what we have as the schema of our scored dataframe:\n",
    "\n",
    " - `userId`\n",
    " - `prediction`\n",
    "\n",
    "And this is what we have as the schema of our ingestion dataset:\n",
    "\n",
    " - `_$TENANTID.userid`\n",
    " - `_$TENANTID.propensity`\n",
    "\n",
    "Although it may look obvious to us, we still need to let the platform know which fields maps to what. This can be achieved using the Data Prep service which allows you to specify a set of transformations to map one field to another. In our case the transformation is pretty simple, we just need to match the schemas without making any changes, but you can do a lot more extensive transformations using this service if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "02d502416f624beca9bce3534d641040"
   },
   "outputs": [],
   "source": [
    "from aepp import dataprep\n",
    "\n",
    "dataprep_conn = dataprep.DataPrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0830be12becd4e4ca898bd3849a4c44a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a15b7a9686c4468aacc642b7019110ae'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_res = dataprep_conn.createMappingSet(\n",
    "  schemaId=schema_id,\n",
    "  validate=True,\n",
    "  mappingList=[\n",
    "    {\n",
    "      \"sourceType\": \"ATTRIBUTE\",\n",
    "      \"source\": \"prediction\",\n",
    "      \"destination\": f\"_{tenant_id}.propensity\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceType\": \"ATTRIBUTE\",\n",
    "      \"source\": \"userId\",\n",
    "      \"destination\": f\"_{tenant_id}.userid\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "mapping_id = mapping_res[\"id\"]\n",
    "mapping_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50d0e79c4e10419eb326a2d5aa992f40"
   },
   "source": [
    "At that point we have everything we need to create a __Data Flow__. A data flow is the \"recipe\" that describes where the data comes from and where it should end up. We can also specify how often checks happen to find new data, but it cannot be lower than 15 minutes currently for platform stability reasons. A data flow is tied to a flow spec ID which contains the instructions for transfering data in an optimized way between a source and destination.\n",
    "\n",
    "For reference, here is a list of all the flow specs available for the most popular cloud storage accounts (these IDs are global across every single customer account and sandbox):\n",
    "    \n",
    "| Cloud Storage Type | Flow Spec ID | Flow Spec Name \n",
    "|-----------------------|--------------------------------------|------------------\n",
    "| Amazon S3 | 9753525b-82c7-4dce-8a9b-5ccfce2b9876 | CloudStorageToAEP \n",
    "| Azure Blob Storage | 14518937-270c-4525-bdec-c2ba7cce3860 | CRMToAEP \n",
    "| Azure Data Lake | 9753525b-82c7-4dce-8a9b-5ccfce2b9876 | CloudStorageToAEP \n",
    "| Data Landing Zone | 9753525b-82c7-4dce-8a9b-5ccfce2b9876 | CloudStorageToAEP \n",
    "| Google Cloud Storage | 9753525b-82c7-4dce-8a9b-5ccfce2b9876 | CloudStorageToAEP \n",
    "| SFTP | 9753525b-82c7-4dce-8a9b-5ccfce2b9876 | CloudStorageToAEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "d416b128266c41898cc51fa66dfb67df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9753525b-82c7-4dce-8a9b-5ccfce2b9876'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_spec = flow_conn.getFlowSpecs(\"name==CloudStorageToAEP\")\n",
    "flow_spec_id = flow_spec[0][\"id\"]\n",
    "flow_spec_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5efc3c11b6dc448089817cd3f9c3a266"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c4103810-5443-401e-8812-a8ec9eb3f6b5'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# TODO: cleanup in aepp, first param should not be required\n",
    "flow_res = flow_conn.createFlow(flow_spec_id, obj={\n",
    "  \"name\": f\"[CMLE][Week4] DLZ to AEP for user propensity (created by {username})\",\n",
    "  \"flowSpec\": {\n",
    "      \"id\": flow_spec_id,\n",
    "      \"version\": \"1.0\"\n",
    "  },\n",
    "  \"sourceConnectionIds\": [\n",
    "      source_connection_id\n",
    "  ],\n",
    "  \"targetConnectionIds\": [\n",
    "      target_connection_id\n",
    "  ],\n",
    "  \"transformations\": [\n",
    "      {\n",
    "          \"name\": \"Mapping\",\n",
    "          \"params\": {\n",
    "              \"mappingId\": mapping_id,\n",
    "              \"mappingVersion\": 0\n",
    "          }\n",
    "      }\n",
    "  ],\n",
    "  \"scheduleParams\": {\n",
    "      \"startTime\": str(int(time.time())),\n",
    "      \"frequency\": \"minute\",\n",
    "      \"interval\": \"15\"\n",
    "  }\n",
    "})\n",
    "dataflow_id = flow_res[\"id\"]\n",
    "dataflow_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cab8263d44b245648fe81b50a5f8ca02"
   },
   "source": [
    "Note that the name of the transformation has to be set to `Mapping` or the job will fail.\n",
    "\n",
    "You should be able to see your Data Flow in the UI at the link below, and you may see some executions depending on when you check since it runs on a schedule and will still show the run even if there was no data to process, as shown in the screenshot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "85f016378ee744b9bf0fa9823c282ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Flow created as ID c4103810-5443-401e-8812-a8ec9eb3f6b5 available under https://experience-stage.adobe.com/#/@cloudmlecosystem/sname:cmle-datarobot/platform/source/dataflows/c4103810-5443-401e-8812-a8ec9eb3f6b5\n"
     ]
    }
   ],
   "source": [
    "dataflow_link = get_ui_link(tenant_id, \"source/dataflows\", dataflow_id)\n",
    "print(f\"Data Flow created as ID {dataflow_id} available under {dataflow_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96036ab007784ffc93981d3759917ddc"
   },
   "source": [
    "Note: If you would like to switch to a different cloud storage, you need to update the flow_spec_id variable above to the matching value in the table mentioned earlier in this section. You can refer to the name from the table above to find out the ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d700b8f7034947c986bdd43d80f8f626"
   },
   "source": [
    "## 2.3 Ingest the scored users into the Unified Profile\n",
    "\n",
    "At that point we have successfully setup a Data Flow that is listening on any files being written to the DLZ under our import path, and will automatically convert it to XDM and deliver it to our dataset where they will be picked up for ingestion into the Unified Profile. All that is left to do is to actually deliver the data. For that, we refer to our Spark dataframe computed earlier `df_to_ingest` and we need to write it to the DLZ under the afore-mentioned folder.\n",
    "\n",
    "Before we can do that we need to update the credentials, because we'll be writing to a different container in the DLZ (`dlz-user-container` instead of `dlz-destination`) so the credentials are different. This should not cause an issue with the lazy computation of our dataframe because we used `.cache()` to cache it so it should already be in memory right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a4947197fe4245f88cd9428cf69a5d64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abfss://dlz-user-container@sndbxdtlnd5m4pyy87h4d027.dfs.core.windows.net/'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlz_path = configureSparkSessionAndGetPath(dlz_credentials)\n",
    "dlz_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68996352ccbc47efa0766f80c9f91719"
   },
   "source": [
    "Now let's determine the full path where we need to write. We use a similar convention as when the data as been egressed, which is `cmle/ingress/$DATASETID/exportTime=$EXPORTTIME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a514d42813f14b1fb8678ae046339004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abfss://dlz-user-container@sndbxdtlnd5m4pyy87h4d027.dfs.core.windows.net/cmle/ingress/663bb3d786def12c699779a1/exportTime=20240509153003/'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "scoring_export_time = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "output_path = f\"{dlz_path}{import_path}/{ingestion_dataset_id}/exportTime={scoring_export_time}/\"\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d2a468dbf1a495ab83c5ac3b0502eec"
   },
   "source": [
    "Now we just need to write the datafrae to this output path. Note that because we chose delimited format in our Data Flow setup, we're just going to write the resulting files as CSV format, and include the header so the transformation knows which field is which column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "b0aca281dbba46b98669f7f86df8250d"
   },
   "outputs": [],
   "source": [
    "df_to_ingest \\\n",
    "  .write \\\n",
    "  .option(\"header\", True) \\\n",
    "  .format(\"csv\") \\\n",
    "  .save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86cf6053d3694398a935f3b24fbb63f9"
   },
   "source": [
    "Because the Data Flow is executed asynchronously every 15 minutes, it may take a few minutes before the data is ingested in the dataset. We can check the status of the runs below until we can see the run has successfully completed to check some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "202197559c53479e873a4d8607df1f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No runs completed yet for flow c4103810-5443-401e-8812-a8ec9eb3f6b5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# TODO: handle that more gracefully in aepp\n",
    "finished = False\n",
    "while not finished:\n",
    "  try:\n",
    "      runs = flow_conn.getRuns(prop=f\"flowId=={dataflow_id}\")\n",
    "      for run in runs:\n",
    "          run_id = run[\"id\"]\n",
    "          run_started_at = run[\"metrics\"][\"durationSummary\"][\"startedAtUTC\"]\n",
    "          run_ended_at = run[\"metrics\"][\"durationSummary\"][\"completedAtUTC\"]\n",
    "          run_duration_secs = (run_ended_at - run_started_at) / 1000\n",
    "          run_size_mb = run[\"metrics\"][\"sizeSummary\"][\"outputBytes\"] / 1024. / 1024.\n",
    "          run_num_rows = run[\"metrics\"][\"recordSummary\"][\"outputRecordCount\"]\n",
    "          run_num_files = run[\"metrics\"][\"fileSummary\"][\"outputFileCount\"]\n",
    "          print(f\"Run ID {run_id} completed with: duration={run_duration_secs} secs; size={run_size_mb} MB; num_rows={run_num_rows}; num_files={run_num_files}\")\n",
    "      finished = True\n",
    "  except Exception as e:\n",
    "      print(f\"No runs completed yet for flow {dataflow_id}\")\n",
    "      time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22c4325f302c48169536ed27af8fb9a7"
   },
   "source": [
    "Once this is done, you should be able to go back in your dataset at the same link as before and see a batch created successfully in it. You should also notice for that batch that the records ingested will also show up under **Existing Profile Fragments** which means they have been ingested in the Unified Profile successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46aa8fc877fc4c8688d067caea139983"
   },
   "source": [
    "## 2.4 Storing the scoring dataset ID in the configuration\n",
    "\n",
    "Now that we got everything working, we just need to save the ingestion_dataset_id variable in the original configuration file, so we can refer to it in the following weekly assignment. To do that, execute the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "71d76792486c43b69ade1657df5ea34a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'config.ini',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'asset_id': '3b0346fe-24e6-42cf-9f33-38d3b9bc87a7'}"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.set(\"Platform\", \"scoring_dataset_id\", ingestion_dataset_id)\n",
    "config_string = io.StringIO()\n",
    "config.write(config_string)\n",
    "project.save_data(file_name=\"config.ini\", data=config_string.getvalue(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a730c65251ad47b7b9ee2b5d09d8e8e2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
