{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf0a859d-14d0-452f-b0c6-c6926ffe69a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Scope of Notebook\n",
    "\n",
    "This notebook allows you to plug in your featuried dataset from the previous week into an ml model, in this case we use random forest.  You will then be able to store the trained model in mlflow and calculate performance characteristics around the model like AUC and accuracy.  The advanced section of this notebook outlines how to retrieve the best set of hyperparameters to certify a model for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22510862-b275-4ff0-8c04-f5a965a3eed7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![ml-model-train](../media/CMLE-Notebooks-Week3-Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5416e3-ff10-4c54-a9b4-6ab4a0b5b788",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac6b4958-397f-4aad-9d81-afb1d45ea0b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook requires some configuration data to properly authenticate to your Adobe Experience Platform instance. You should be able to find all the values required above by following the Setup section of the **README**.\n",
    "\n",
    "The next cell will be looking for your configuration file under your **ADOBE_HOME** path to fetch the values used throughout this notebook. See more details in the Setup section of the **README** to understand how to create your configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03b1588-2e38-44da-a7a6-d8aff3bf72eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import ConfigParser\n",
    "  \n",
    "config = ConfigParser()\n",
    "config_path = os.path.join(os.environ[\"ADOBE_HOME\"], \"conf\", \"config.ini\")\n",
    "if not os.path.exists(config_path):\n",
    "  raise Exception(f\"Looking for configuration under {config_path} but config not found, please verify path\")\n",
    "config.read(config_path)\n",
    "  \n",
    "ims_org_id = config.get(\"Platform\", \"ims_org_id\")\n",
    "sandbox_name = config.get(\"Platform\", \"sandbox_name\")\n",
    "environment = config.get(\"Platform\", \"environment\")\n",
    "client_id = config.get(\"Authentication\", \"client_id\")\n",
    "client_secret = config.get(\"Authentication\", \"client_secret\")\n",
    "scopes = config.get(\"Authentication\", \"scopes\")\n",
    "dataset_id = config.get(\"Platform\", \"dataset_id\")\n",
    "featurized_dataset_id = config.get(\"Platform\", \"featurized_dataset_id\")\n",
    "export_path = config.get(\"Cloud\", \"export_path\")\n",
    "import_path = config.get(\"Cloud\", \"import_path\")\n",
    "data_format = config.get(\"Cloud\", \"data_format\")\n",
    "compression_type = config.get(\"Cloud\", \"compression_type\")\n",
    "model_name = config.get(\"Cloud\", \"model_name\")\n",
    "datarobot_key = config.get(\"DataRobot\", 'datarobot_key')\n",
    "datarobot_endpoint = config.get(\"DataRobot\", 'datarobot_endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109006a1-8d31-4515-b9cc-143fe8ddd4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: cmenguy@adobe.com\n",
      "Unique ID: cmenguy_adobe_com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "unique_id = s = re.sub(\"[^0-9a-zA-Z]+\", \"_\", username)\n",
    "\n",
    "print(f\"Username: {username}\")\n",
    "print(f\"Unique ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf3303b-e086-4638-9aa2-7d62bdca24bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before we run anything, make sure to install the following required libraries for this notebook. They are all publicly available libraries and the latest version should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44dd3429-850b-4679-82e3-dfea9688993b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aepp\r\n",
      "  Downloading aepp-0.2.9-py3-none-any.whl (120 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |██▊                             | 10 kB 30.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 20 kB 14.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 30 kB 20.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 40 kB 10.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 51 kB 11.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 61 kB 13.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 71 kB 10.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 81 kB 9.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 92 kB 10.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 102 kB 10.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 112 kB 10.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 120 kB 10.9 MB/s \r\n",
      "\u001b[?25hCollecting pathlib2\r\n",
      "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting pathlib\r\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: PyJWT in /databricks/python3/lib/python3.9/site-packages (from aepp) (2.6.0)\r\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from aepp) (1.4.2)\r\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from aepp) (2.27.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->aepp) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->aepp) (2021.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /databricks/python3/lib/python3.9/site-packages (from pandas->aepp) (1.21.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->aepp) (1.16.0)\r\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /databricks/python3/lib/python3.9/site-packages (from PyJWT->aepp) (3.4.8)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=3.4.0->PyJWT->aepp) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.4.0->PyJWT->aepp) (2.21)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->aepp) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->aepp) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->aepp) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->aepp) (2021.10.8)\r\n",
      "Installing collected packages: pathlib2, pathlib, aepp\r\n",
      "Successfully installed aepp-0.2.9 pathlib-1.0.1 pathlib2-2.3.7.post1\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba66f562-f854-4b23-9191-cf0cc1b7ed77/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting adlfs\r\n",
      "  Downloading adlfs-2023.1.0-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: fsspec>=2021.10.1 in /databricks/python3/lib/python3.9/site-packages (from adlfs) (2022.2.0)\r\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46\r\n",
      "  Downloading azure_datalake_store-0.0.52-py2.py3-none-any.whl (61 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |█████▎                          | 10 kB 35.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 20 kB 44.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 30 kB 55.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 40 kB 19.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 51 kB 12.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 61 kB 14.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61 kB 333 kB/s \r\n",
      "\u001b[?25hCollecting azure-storage-blob>=12.12.0\r\n",
      "  Downloading azure_storage_blob-12.15.0-py3-none-any.whl (387 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |▉                               | 10 kB 32.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 20 kB 43.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 30 kB 54.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 40 kB 56.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 51 kB 60.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 61 kB 66.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 71 kB 71.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 81 kB 75.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 92 kB 21.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 102 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 112 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 122 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 133 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 143 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 153 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 163 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 174 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 184 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 194 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 204 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 215 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 225 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 235 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 245 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 256 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 266 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 276 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 286 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 296 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 307 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 317 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 327 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 337 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 348 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 358 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 368 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 378 kB 23.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 387 kB 23.2 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.23.1 in /databricks/python3/lib/python3.9/site-packages (from adlfs) (1.26.1)\r\n",
      "Collecting aiohttp>=3.7.0\r\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 33.3 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 38.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30 kB 47.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40 kB 51.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51 kB 47.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61 kB 51.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 71 kB 55.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 81 kB 59.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92 kB 63.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 122 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 133 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 143 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 153 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 163 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 174 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 184 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 204 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 215 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 235 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 245 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 256 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 266 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 276 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 286 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 296 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 307 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 317 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 327 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 337 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 348 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 358 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 368 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 378 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 389 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 399 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 409 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 419 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 430 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 440 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 450 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 460 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 471 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 481 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 491 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 501 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 512 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 522 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 532 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 542 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 552 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 563 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 573 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 583 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 593 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 604 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 614 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 624 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 634 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 645 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 655 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 665 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 675 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 686 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 696 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 706 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 716 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 727 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 737 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 747 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 757 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 768 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 778 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 788 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 798 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 808 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 819 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 829 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 839 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 849 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 860 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 870 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 880 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 890 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 901 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 911 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 921 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 931 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 942 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 952 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 962 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 972 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 983 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 993 kB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.0 MB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.0 MB 66.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0 MB 66.5 MB/s \r\n",
      "\u001b[?25hCollecting azure-identity\r\n",
      "  Downloading azure_identity-1.12.0-py3-none-any.whl (135 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |██▍                             | 10 kB 39.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 20 kB 49.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 30 kB 61.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 40 kB 68.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 51 kB 73.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 61 kB 80.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 71 kB 85.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 81 kB 88.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 92 kB 92.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 102 kB 94.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 112 kB 94.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 122 kB 94.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 133 kB 94.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 135 kB 94.6 MB/s \r\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\r\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |██                              | 10 kB 37.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 20 kB 47.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 30 kB 59.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 40 kB 66.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 51 kB 72.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 61 kB 78.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 71 kB 82.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 81 kB 85.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 92 kB 89.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 102 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 112 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 122 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 133 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 143 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 153 kB 91.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 158 kB 91.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp>=3.7.0->adlfs) (2.0.4)\r\n",
      "Collecting yarl<2.0,>=1.0\r\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▎                              | 10 kB 45.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 20 kB 55.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 30 kB 66.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 40 kB 72.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 51 kB 76.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 61 kB 83.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 71 kB 87.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 81 kB 90.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 92 kB 94.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 102 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 112 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 122 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 133 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 143 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 153 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 163 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 174 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 184 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 194 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 204 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 215 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 225 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 235 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 245 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 256 kB 97.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 264 kB 97.2 MB/s \r\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\r\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n",
      "Collecting aiosignal>=1.1.2\r\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Collecting multidict<7.0,>=4.5\r\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |██▉                             | 10 kB 39.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 20 kB 49.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 30 kB 60.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 40 kB 68.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 51 kB 72.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 61 kB 79.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 71 kB 82.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 81 kB 87.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 92 kB 90.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 102 kB 92.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 112 kB 92.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 114 kB 92.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp>=3.7.0->adlfs) (21.4.0)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (1.16.0)\r\n",
      "Requirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.23.1->adlfs) (4.1.1)\r\n",
      "Collecting adal>=0.4.2\r\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |██████                          | 10 kB 39.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 20 kB 49.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 30 kB 60.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 40 kB 68.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 51 kB 74.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: cffi in /databricks/python3/lib/python3.9/site-packages (from azure-datalake-store<0.1,>=0.0.46->adlfs) (1.15.0)\r\n",
      "Requirement already satisfied: python-dateutil<3,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from adal>=0.4.2->azure-datalake-store<0.1,>=0.0.46->adlfs) (2.8.2)\r\n",
      "Requirement already satisfied: cryptography>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from adal>=0.4.2->azure-datalake-store<0.1,>=0.0.46->adlfs) (3.4.8)\r\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from adal>=0.4.2->azure-datalake-store<0.1,>=0.0.46->adlfs) (2.6.0)\r\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.9/site-packages (from azure-storage-blob>=12.12.0->adlfs) (0.6.1)\r\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi->azure-datalake-store<0.1,>=0.0.46->adlfs) (2.21)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.1->adlfs) (2021.10.8)\r\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0\r\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\r\n",
      "Collecting msal<2.0.0,>=1.12.0\r\n",
      "  Downloading msal-1.21.0-py2.py3-none-any.whl (89 kB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |███▋                            | 10 kB 33.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 20 kB 42.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 30 kB 53.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 40 kB 60.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 51 kB 65.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 61 kB 70.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 71 kB 74.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 81 kB 78.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 89 kB 9.5 MB/s \r\n",
      "\u001b[?25hCollecting portalocker<3,>=1.0\r\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: portalocker, multidict, msal, frozenlist, yarl, msal-extensions, async-timeout, aiosignal, adal, azure-storage-blob, azure-identity, azure-datalake-store, aiohttp, adlfs\r\n",
      "Successfully installed adal-1.2.7 adlfs-2023.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 azure-datalake-store-0.0.52 azure-identity-1.12.0 azure-storage-blob-12.15.0 frozenlist-1.3.3 msal-1.21.0 msal-extensions-1.0.0 multidict-6.0.4 portalocker-2.7.0 yarl-1.8.2\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-ba66f562-f854-4b23-9191-cf0cc1b7ed77/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install aepp\n",
    "!pip install adlfs\n",
    "!pip install s3fs\n",
    "!pip install fsspec\n",
    "%pip install datarobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe533d43-2e75-4d3b-9310-54e4e799d44f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before any calls can take place, we need to configure the library and setup authentication credentials. For this you'll need the following piece of information. For information about how you can get these, please refer to the `Setup` section of the **Readme**:\n",
    "- Client ID\n",
    "- Client secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc26cd22-8eb5-460b-9766-0e266b094d55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import aepp\n",
    "\n",
    "aepp.configure(\n",
    "  environment=environment,\n",
    "  sandbox=sandbox_name,\n",
    "  org_id=ims_org_id,\n",
    "  scopes=scopes, \n",
    "  secret=client_secret,\n",
    "  client_id=client_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2b4524-29c9-4bce-a81e-cc1c4ef1c498",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Running a model on AEP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a12e05-2587-4ffb-a80b-a75a6f173edf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the previous week we generated our featurized data in the Data Landing Zone under the `dlz-destination` container. We can now read it so we can use it to train our ML model. Because this data can be pretty big, we want to first read it via a Spark dataframe, so we can then use a sample of it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147b3214-6fcc-4991-9b30-5377e34e3095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The featurized data exported into the Data Landing Zone is under the format **cmle/egress/$DATASETID/exportTime=$EXPORTTIME**. We know the dataset ID which is in your config under `featurized_dataset_id` so we're just missing the export time so we know what to read. To get that we can simply list files in the DLZ and find what the value is. The first step is to retrieve the credentials for the DLZ related to the destination container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af95a9a8-be52-4f6f-bcec-6ff4c84e597f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from aepp import flowservice\n",
    "\n",
    "flow_conn = flowservice.FlowService()\n",
    "credentials = flow_conn.getLandingZoneCredential(dlz_type='dlz_destination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7798bac9-4baf-45d4-bceb-60eed1f5add9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we use some Python libraries to authenticate and issue listing commands so we can get the paths and extract the time from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc50edb-5674-4572-8936-3724ea893127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using featurized data export time of 20230401140556\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "from fsspec import AbstractFileSystem\n",
    "\n",
    "def getDLZFSPath(credentials: dict):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        aws_credentials = {\n",
    "            'key' : credentials['credentials']['awsAccessKeyId'],\n",
    "            'secret' : credentials['credentials']['awsSecretAccessKey'],\n",
    "            'token' : credentials['credentials']['awsSessionToken']\n",
    "        }\n",
    "        return fsspec.filesystem('s3', **aws_credentials), credentials['dlzPath']['bucketName']\n",
    "    else:\n",
    "        abs_credentials = {\n",
    "            'account_name' : credentials['storageAccountName'],\n",
    "            'sas_token' : credentials['SASToken']\n",
    "        }\n",
    "        return fsspec.filesystem('abfss', **abs_credentials), credentials['containerName']\n",
    "    \n",
    "def getDLZDataPath(credentials):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        aws_buket = credentials['dlzPath']['bucketName']\n",
    "        dlz_folder = credentials['dlzPath']['dlzFolder']\n",
    "        return f\"s3a://${aws_buket}/{dlz_folder}/\"\n",
    "    else:\n",
    "        dlz_storage_account = credentials['storageAccountName']\n",
    "        dlz_container = credentials['containerName']\n",
    "        return f\"abfss://{dlz_container}@{dlz_storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "\n",
    "def get_export_time(fs: AbstractFileSystem, container_name: str, base_path: str, dataset_id: str):\n",
    "  featurized_data_base_path = f\"{container_name}/{base_path}/{dataset_id}\"\n",
    "  featurized_data_export_paths = fs.ls(featurized_data_base_path)\n",
    "  \n",
    "  if len(featurized_data_export_paths) == 0:\n",
    "    raise Exception(f\"Found no exports for featurized data from dataset ID {dataset_id} under path {featurized_data_base_path}\")\n",
    "  elif len(featurized_data_export_paths) > 1:\n",
    "    print(f\"Found {len(featurized_data_export_paths)} exports from dataset dataset ID {dataset_id} under path {featurized_data_base_path}, using most recent one\")\n",
    "  \n",
    "  featurized_data_export_path = featurized_data_export_paths[-1]\n",
    "  featurized_data_export_time = featurized_data_export_path.strip().split(\"/\")[-1].split(\"=\")[-1]\n",
    "  return featurized_data_export_time\n",
    "\n",
    "\n",
    "fs, container = getDLZFSPath(credentials)\n",
    "\n",
    "\n",
    "export_time = get_export_time(fs, container, export_path, featurized_dataset_id)\n",
    "print(f\"Using featurized data export time of {export_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "344f7396-c122-47c7-83ae-2096b452054b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "At that point we're ready to read this data. We're using Spark since it could be pretty large as we're not doing any sampling. \n",
    "Based on the provisioned account Landing Zone could be either configured to use azure or aws, \n",
    "in case of azure following properties will be used to authenticate using SAS:\n",
    "- `fs.azure.account.auth.type.$ACCOUNT.dfs.core.windows.net` should be set to `SAS`.\n",
    "- `fs.azure.sas.token.provider.type.$ACCOUNT.dfs.core.windows.net` should be set to `org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider`.\n",
    "- `fs.azure.sas.fixed.token.$ACCOUNT.dfs.core.windows.net` should be set to the SAS token retrieved earlier.\n",
    "\n",
    "in case of aws following properties will be used to access data stored in s3:\n",
    "- `fs.s3a.access.key` and `spark.hadoop.fs.s3a.access.key` should be the s3 access key\n",
    "- `fs.s3a.secret.key` and `spark.hadoop.fs.s3a.secret.key` should be the s3 secret\n",
    "- `fs.s3a.session.token` and `spark.hadoop.fs.s3a.session.token` should be set to s3 session token\n",
    "- `fs.s3a.aws.credentials.provider` and `spark.hadoop.fs.s3a.aws.credentials.provider` should be set to `org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider`\n",
    "- `fs.s3.impl` and `spark.hadoop.fs.s3.impl` should be set to `org.apache.hadoop.fs.s3a.S3AFileSystem`\n",
    "\n",
    "\n",
    "The above properties are calculated based on the landing zone credentials, following util method will set these up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configureSparkSessionAndGetPath(credentials):\n",
    "    if 'dlzProvider' in credentials.keys() and ['Amazon', 's3'] in credentials['dlzProvider']:\n",
    "        aws_key = credentials['credentials']['awsAccessKeyId']\n",
    "        aws_secret = credentials['credentials']['awsSecretAccessKey']\n",
    "        aws_token = credentials['credentials']['awsSessionToken']\n",
    "        aws_buket = credentials['dlzPath']['bucketName']\n",
    "        dlz_folder = credentials['dlzPath']['dlzFolder']\n",
    "        spark.conf.set(\"fs.s3a.access.key\", aws_key)\n",
    "        spark.conf.set(\"fs.s3a.secret.key\", aws_secret)\n",
    "        spark.conf.set(\"fs.s3a.session.token\", aws_token)\n",
    "        spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "        spark.conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_key)\n",
    "        spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_secret)\n",
    "        spark.conf.set(\"fs.s3a.session.token\", aws_token)\n",
    "        return f\"s3a://${aws_buket}/{dlz_folder}/\"\n",
    "    else:\n",
    "        dlz_storage_account = credentials['storageAccountName']\n",
    "        dlz_sas_token = credentials['SASToken']\n",
    "        dlz_container = credentials['containerName']\n",
    "        spark.conf.set(f\"fs.azure.account.auth.type.{dlz_storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "        spark.conf.set(f\"fs.azure.sas.token.provider.type.{dlz_storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "        spark.conf.set(f\"fs.azure.sas.fixed.token.{dlz_storage_account}.dfs.core.windows.net\", dlz_sas_token)\n",
    "        return f\"abfss://{dlz_container}@{dlz_storage_account}.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bace8783-6773-42e7-86be-1f5134654453",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- eventType: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- subscriptionOccurred: long (nullable = true)\n",
      " |-- emailsReceived: long (nullable = true)\n",
      " |-- emailsOpened: long (nullable = true)\n",
      " |-- emailsClicked: long (nullable = true)\n",
      " |-- productsViewed: long (nullable = true)\n",
      " |-- propositionInteracts: long (nullable = true)\n",
      " |-- propositionDismissed: long (nullable = true)\n",
      " |-- webLinkClicks: long (nullable = true)\n",
      " |-- minutes_since_emailSent: integer (nullable = true)\n",
      " |-- minutes_since_emailOpened: integer (nullable = true)\n",
      " |-- minutes_since_emailClick: integer (nullable = true)\n",
      " |-- minutes_since_productView: integer (nullable = true)\n",
      " |-- minutes_since_propositionInteract: integer (nullable = true)\n",
      " |-- minutes_since_propositionDismiss: integer (nullable = true)\n",
      " |-- minutes_since_linkClick: integer (nullable = true)\n",
      " |-- random_row_number_for_user: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init spark session for provisioned DLZ and get the base path (fs3://bucket_name/folder or abfss://container@account/)\n",
    "cloud_base_path = configureSparkSessionAndGetPath(credentials)\n",
    "\n",
    "input_path = cloud_base_path + f\"{export_path}/{featurized_dataset_id}/exportTime={export_time}/\"\n",
    "\n",
    "#Let's put that in practice and create a Spark dataframe containing the entire featurized data:\n",
    "df = spark.read.parquet(input_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153894ac-e08c-4cd7-b43c-e9db5a834384",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can then sample it to keep only a portion of the data for training before we bring the data in memory for use in the `scikit-learn` library. Here we're just going to use a sampling ratio of 50%, but you are welcome to use a bigger or smaller ratio. We use sampling **without** replacement to ensure the same profiles don't get picked up multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90bca5c-16d6-4603-b85d-ce4b5fad185b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampling_ratio = 0.5\n",
    "df = df.sample(withReplacement=False, fraction=sampling_ratio)\n",
    "\n",
    "df_train=df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e40df83-de43-4588-bf61-abb0b9bb77ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 Creating baseline models in DataRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c69d7233-f138-484d-b5f8-a768a6ed8541",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before doing any ML we can look at summary statistics to understand the structure of the data, and what kind of algorithm(s) might be suited to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2a5ca8-ab42-4ca3-a221-453eb31f59d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: DataFrame[summary: string, userId: string, eventType: string, subscriptionOccurred: string, emailsReceived: string, emailsOpened: string, emailsClicked: string, productsViewed: string, propositionInteracts: string, propositionDismissed: string, webLinkClicks: string, minutes_since_emailSent: string, minutes_since_emailOpened: string, minutes_since_emailClick: string, minutes_since_productView: string, minutes_since_propositionInteract: string, minutes_since_propositionDismiss: string, minutes_since_linkClick: string, random_row_number_for_user: string]"
     ]
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f70ef9-2dcf-44a1-b811-1a7b646b598c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To keep the model name unique we append the username to the model name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35dd9c15-f90f-4f5d-8168-649a608eff42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = f\"{model_name}_{unique_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "288890c1-9218-4a35-90e1-eb4e47540fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In order to feed data to our model, we need to do a few preparation steps:\n",
    "- Separate the target variable (which in our case is whether a subscription occured or not) from the other variables.\n",
    "- Split the data into a training and test set so we can evaluate our model performance down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "## 1.2. Connect to DataRobot\n",
    "\n",
    "Read more about different options for connecting to DataRobot from the client: https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html\n",
    "\n",
    "**To connect to DataRobot,** we just need to provide our API Token (found in Developer Tools) and the endpoint.\n",
    "\n",
    "The endpoint for VPC installs will be different. The format will follow:\n",
    "**https://{datarobot.example.com}/api/v2** <br>\n",
    "See On-premise section in: https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#retrieve-the-api-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "\n",
    "dr.Client(\n",
    "    token=datarobot_key, \n",
    "    endpoint=datarobot_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "### Upload dataset to DataRobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dr.Dataset.create_from_in_memory_data(data_frame=df_train)\n",
    "# Update the dataset name in the AI Catalog\n",
    "new_dataset.modify(name=model_name)\n",
    "\n",
    "# Output the new dataset ID\n",
    "print(\"Dataset ID of our new AI Catalog dataset: \" + new_dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "Every dataset in the AI Catalog is given a unique dataset ID. This can be found in the URL as well as the dataset metadata in the UI. For example, the dataset url 'https://app.datarobot.com/ai-catalog/64cfc12417441cd3242e99ec' contains the dataset ID: 64cfc12417441cd3242e99ec. The ID can be used to retrieve the AI Catalog dataset object with the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "### 1.3. Create a DataRobot project from a AI Catalog Dataset\n",
    "\n",
    "Ref: https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.28.2/reference/modeling/project.html#create-a-project\n",
    "\n",
    "We can create DataRobot projects directly from:\n",
    "\n",
    "* A dataset in AI Catalog (using the dataset's ID in DataRobot)\n",
    "* A pandas dataframe (don't need to write back to data source or disk)\n",
    "* Data sources\n",
    "\n",
    "Note: Each created project is associated with a unique Project ID. To use the API for downstream analytics, we can use the project ID to pull the project of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dr.Project.create_from_dataset(dataset_id=new_dataset.id,\n",
    "                                            project_name=model_name, max_wait=600)\n",
    "\n",
    "# Quick link to the DataRobot project you just created\n",
    "print(\n",
    "    \"DataRobot Project URL: \" + project.get_uri()\n",
    ")\n",
    "print(\"Project ID: \" + project.id)\n",
    "\n",
    "partitioning = dr.GroupCV(partition_key_cols=[\"userId\"], reps=5, holdout_pct=20)\n",
    "# featurelist = project.create_featurelist('modelling', ['subscription'])\n",
    "\n",
    "advanced_options = dr.AdvancedOptions(\n",
    "    feature_discovery_supervised_feature_reduction=False,) \n",
    "\n",
    "project.analyze_and_model(\n",
    "            target='subscriptionOccurred',\n",
    "            mode=dr.AUTOPILOT_MODE.QUICK,\n",
    "            # featurelist_id=featurelist.id,\n",
    "            partitioning_method=partitioning,\n",
    "            worker_count=-1,\n",
    "            max_wait=600*600) #Entering -1 uses all available workers\n",
    "\n",
    "# Setting timeout=None as the feature engineering and reduction for this dataset is extensive\n",
    "project.wait_for_autopilot(timeout=60*60*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "### Deploy Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the prediction server ID and model ID from your DataRobot project\n",
    "default_prediction_server_id = [dps for dps in dr.PredictionServer.list()][0]\n",
    "\n",
    "# Create a new deployment from best model\n",
    "\n",
    "\n",
    "rec = dr.ModelRecommendation.get(\n",
    "                project.id,\n",
    "                recommendation_type=dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT)\n",
    "\n",
    "model = rec.get_model()\n",
    "\n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    model.id,\n",
    "    label='Adobe_Subsription',\n",
    "    description='Adobe_Subsription',\n",
    "    default_prediction_server_id=default_prediction_server_id.id\n",
    ")\n",
    "\n",
    "deployment_id = deployment.id\n",
    "# Display created deployment\n",
    "print(deployment)\n",
    "print(deployment.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786ee31c-9b36-4f94-a8b8-1c5d8aca6ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.4 Saving the DataRobot deplyment ID to configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b90e437-2bce-439c-9319-8dd1da72a130",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we got everything working, we just need to save the updated `deployment_id` variable in the original configuration file, so we can refer to it in the following weekly assignments. To do that, execute the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874d109e-a06b-4b4b-bd00-a28e3f5f3be2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config.set(\"DataRobot\", \"datarobot_deployment_id\", deployment_id)\n",
    "\n",
    "with open(config_path, \"w\") as configfile:\n",
    "    config.write(configfile)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "Week3Notebook",
   "notebookOrigID": 3680348664249912,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
