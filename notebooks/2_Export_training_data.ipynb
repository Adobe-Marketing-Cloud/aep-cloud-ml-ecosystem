{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77418fa1",
   "metadata": {},
   "source": [
    "# Export training data\n",
    "\n",
    "Now that our featurized data is in a dataset, we need to bring it out to an external cloud storage filesystem from which the ML model training and scoring will be performed. \n",
    "\n",
    "For the purposes of this notebook we will be using the [Data Landing Zone (DLZ)](https://experienceleague.adobe.com/docs/experience-platform/sources/api-tutorials/create/cloud-storage/data-landing-zone.html?lang=en) as the filesystem. Every Adobe Experience Platform customer has a DLZ already setup as an [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs) container. We'll be using that as a delivery mechanism for the featurized data, but this step can be customized to deliver this data to any cloud storage filesystem.\n",
    "\n",
    "To setup the delivery pipeline, we'll be using the [Flow Service for Destinations](https://developer.adobe.com/experience-platform-apis/references/destinations/) which will be responsible for picking up the featurized data and dump it into the DLZ. There's a few steps involved:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [1. Create a source connection](#1-basic-queries)\n",
    "- [2. Create a target connection](#2-exploratory-data-analysis-with-query-service)\n",
    "- [3. Create a data flow](#2-featurization-with-query-service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c199f88",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This notebook requires some configuration parameters to properly authenticate to your Adobe Experience Platform instance. Please follow the instructions in the [**README**](../README.md) to gather the necessary configuration parameters and prepare the [config.ini](../conf/config.ini) file with the specific values for your environment.\n",
    "\n",
    "The next cell will be looking for your configuration file under your **ADOBE_HOME** path to fetch the configuration values that will be used for this notebook. If necessary, modify the `config_path` and/or the `config_file` name to reflect the location of your config file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e88dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import ConfigParser\n",
    "import aepp\n",
    "\n",
    "os.environ[\"ADOBE_HOME\"] = os.path.dirname(os.getcwd())\n",
    "\n",
    "if \"ADOBE_HOME\" not in os.environ:\n",
    "    raise Exception(\"ADOBE_HOME environment variable needs to be set.\")\n",
    "\n",
    "config = ConfigParser()\n",
    "config_file = \"aemassets_config.ini\"\n",
    "#config_path = os.path.join(os.environ[\"ADOBE_HOME\"], \"conf\", config_file)\n",
    "config_path = f\"/Users/jeremypage/Library/CloudStorage/OneDrive-Adobe/Projects/Cloud ML/environments/{config_file}\"\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise Exception(f\"Looking for configuration under {config_path} but config not found, please verify path\")\n",
    "\n",
    "config.read(config_path)\n",
    "\n",
    "export_path = config.get(\"Cloud\", \"export_path\")\n",
    "import_path = config.get(\"Cloud\", \"import_path\")\n",
    "data_format = config.get(\"Cloud\", \"data_format\")\n",
    "\n",
    "aepp.configure(\n",
    "  org_id=config.get(\"Platform\", \"ims_org_id\"),\n",
    "  tech_id=config.get(\"Platform\", \"tech_acct_id\"), \n",
    "  secret=config.get(\"Platform\", \"client_secret\"),\n",
    "  scopes=config.get(\"Platform\", \"scopes\"),\n",
    "  client_id=config.get(\"Platform\", \"client_id\"),\n",
    "  environment=config.get(\"Platform\", \"environment\"),\n",
    "  sandbox=config.get(\"Platform\", \"sandbox_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d49b8",
   "metadata": {},
   "source": [
    "To ensure uniqueness of resources created as part of this notebook, we are using your local username to include in each of the resource titles to avoid conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cea97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "username = os.getlogin()\n",
    "unique_id = s = re.sub(\"[^0-9a-zA-Z]+\", \"_\", username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb3a9d",
   "metadata": {},
   "source": [
    "Helper function to generate link to resource in the UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f07c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ui_link(tenant_id, resource_type, resource_id):\n",
    "    environment = config.get(\"Platform\", \"environment\")\n",
    "    sandbox_name = config.get(\"Platform\", \"sandbox_name\")\n",
    "    if environment == \"prod\":\n",
    "        prefix = f\"https://experience.adobe.com\"\n",
    "    else:\n",
    "        prefix = f\"https://experience-{environment}.adobe.com\"\n",
    "    return f\"{prefix}/#/@{tenant_id}/sname:{sandbox_name}/platform/{resource_type}/{resource_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac5f2f",
   "metadata": {},
   "source": [
    "# 1. Create the Source Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70633ae7",
   "metadata": {},
   "source": [
    "The source connection is responsible for configuring the connection to your Adobe Experience Platform dataset so that the resulting flow will know exactly where to look for the data and in what format.\n",
    "\n",
    "We will use `aepp` to make requests to the Flow Service APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6602d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'512c504a-6531-42c1-8e44-064ccf640eb3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aepp import flowservice\n",
    "flow_conn = flowservice.FlowService()\n",
    "\n",
    "created_dataset_id = config.get(\"Data\", \"featurized_dataset_id\")\n",
    "\n",
    "source_res = flow_conn.createSourceConnectionDataLake(\n",
    "    name=f\"[AIML-FP] Featurized Dataset source connection created by {username}\",\n",
    "    dataset_ids=[created_dataset_id],\n",
    "    format=\"parquet\"\n",
    ")\n",
    "source_connection_id = source_res[\"id\"]\n",
    "source_connection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fed79",
   "metadata": {},
   "source": [
    "# 2. Create the Target Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909b3ee",
   "metadata": {},
   "source": [
    "The target connection is responsible for connecting to the destination filesystem. In our case, we want to connect to the DLZ and specify in what format the data will be stored, as well as the type of compression.\n",
    "\n",
    "Before we can create it however, we need to create a base connection to the DLZ. A base connection is just an instance of a connection spec that details how one authenticates to a particular destination. In our case, because we're using the DLZ which is a known entity internal to Adobe, we can just reference the standard DLZ connection spec ID and create an empty base connection.\n",
    "\n",
    "For reference, here is a list of all the connection specs available for the most popular cloud storage accounts (these IDs are global across every single customer account and sandbox):\n",
    "\n",
    "| Cloud Storage Type    | Connection Spec ID                   |\n",
    "|-----------------------|--------------------------------------|\n",
    "| Amazon S3             | 4fce964d-3f37-408f-9778-e597338a21ee |\n",
    "| Azure Blob Storage    | 6d6b59bf-fb58-4107-9064-4d246c0e5bb2 |\n",
    "| Azure Data Lake       | be2c3209-53bc-47e7-ab25-145db8b873e1 |\n",
    "| Data Landing Zone     | 10440537-2a7b-4583-ac39-ed38d4b848e8 |\n",
    "| Google Cloud Storage  | c5d93acb-ea8b-4b14-8f53-02138444ae99 |\n",
    "| SFTP                  | 36965a81-b1c6-401b-99f8-22508f1e6a26 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e1ff2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e76a9429-4ea2-4b8f-8704-d0c1e371d6d7'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: implement in aepp a way to abstract that\n",
    "connection_spec_id = \"10440537-2a7b-4583-ac39-ed38d4b848e8\"\n",
    "base_connection_res = flow_conn.createConnection(data={\n",
    "    \"name\": f\"[AIML-FP] Base Connection to DLZ created by {username}\",\n",
    "    \"auth\": None,\n",
    "    \"connectionSpec\": {\n",
    "        \"id\": connection_spec_id,\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    "})\n",
    "base_connection_id = base_connection_res[\"id\"]\n",
    "base_connection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be68712",
   "metadata": {},
   "source": [
    "With that base connection, we're ready to create the target connection that will tie to our DLZ directory.  We will configure the target connection using the parameters specified in the `[Cloud]` section of the `config.ini` file:\n",
    "- compression_type\n",
    "- data_format\n",
    "- export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abde3f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d6f1ba66-5c20-4db3-ab57-29160ac38282'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: implement in aepp a way to abstract that\n",
    "target_res = flow_conn.createTargetConnection(\n",
    "    data={\n",
    "        \"name\": f\"[AIML-FP] Data Landing Zone target connection created by {username}\",\n",
    "        \"baseConnectionId\": base_connection_id,\n",
    "        \"params\": {\n",
    "            \"mode\": \"Server-to-server\",\n",
    "            \"compression\": config.get(\"Cloud\", \"compression_type\"),\n",
    "            \"datasetFileType\": config.get(\"Cloud\", \"data_format\"),\n",
    "            \"path\": config.get(\"Cloud\", \"export_path\")\n",
    "        },\n",
    "        \"connectionSpec\": {\n",
    "            \"id\": connection_spec_id,\n",
    "            \"version\": \"1.0\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "target_connection_id = target_res[\"id\"]\n",
    "target_connection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd18a9e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "If you would like to switch to a different cloud storage, you need to update the `connection_spec_id` variable above to the matching value in the table mentioned earlier in this section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07842c",
   "metadata": {},
   "source": [
    "# 3. Create the Data Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58199ff",
   "metadata": {},
   "source": [
    "Now that we have the source and target connections setup, we can construct the data flow. A data flow is the \"recipe\" that describes where the data comes from and where it should end up. We can also specify how often checks happen to find new data, but it cannot be lower than 3 hours currently for platform stability reasons. A data flow is tied to a flow spec ID which contains the instructions for transfering data in an optimized way between a source and destination.\n",
    "\n",
    "For reference, here is a list of all the flow specs available for the most popular cloud storage accounts (these IDs are global across every single customer account and sandbox):\n",
    "\n",
    "| Cloud Storage Type    | Flow Spec ID                         |\n",
    "|-----------------------|--------------------------------------|\n",
    "| Amazon S3             | 269ba276-16fc-47db-92b0-c1049a3c131f |\n",
    "| Azure Blob Storage    | 95bd8965-fc8a-4119-b9c3-944c2c2df6d2 |\n",
    "| Azure Data Lake       | 17be2013-2549-41ce-96e7-a70363bec293 |\n",
    "| Data Landing Zone     | cd2fc47e-e838-4f38-a581-8fff2f99b63a |\n",
    "| Google Cloud Storage  | 585c15c4-6cbf-4126-8f87-e26bff78b657 |\n",
    "| SFTP                  | 354d6aad-4754-46e4-a576-1b384561c440 |\n",
    "\n",
    "\n",
    "In order to execute the data flow, There are two options available to you:\n",
    "1. If you do not want to wait you can do a **adhoc run** to execute it instantly in Section 4.\n",
    "2. **Wait for the first scheduled run**. We selected to have it run every 3 hours, so you may need to wait up to 3 hours.\n",
    "\n",
    "The code below configures the data flow to run ad hoc by default. If you prefer to run the dataflow on a recurring schedule:\n",
    "1. Change the value of on_schedule to `True` before executing the cell below\n",
    "2. Wait up to 3 hours, then execute the cells in Section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f10caed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40a89499-90f1-4988-a38e-55b19d8ce091'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "on_schedule = False\n",
    "if on_schedule:\n",
    "    schedule_params = {\n",
    "        \"interval\": 3,\n",
    "        \"timeUnit\": \"hour\",\n",
    "        \"startTime\": int(time.time())\n",
    "    }\n",
    "else:\n",
    "    schedule_params = {\n",
    "        \"interval\": 1,\n",
    "        \"timeUnit\": \"day\",\n",
    "        \"startTime\": int(time.time() + 60*60*24*365) # Start the schedule far in the future\n",
    "    }\n",
    "\n",
    "\n",
    "flow_spec_id = \"cd2fc47e-e838-4f38-a581-8fff2f99b63a\"\n",
    "flow_obj = {\n",
    "    \"name\": f\"[AIML-FP] Flow for Featurized Dataset to DLZ created by {username}\",\n",
    "    \"flowSpec\": {\n",
    "        \"id\": flow_spec_id,\n",
    "        \"version\": \"1.0\"\n",
    "    },\n",
    "    \"sourceConnectionIds\": [\n",
    "        source_connection_id\n",
    "    ],\n",
    "    \"targetConnectionIds\": [\n",
    "        target_connection_id\n",
    "    ],\n",
    "    \"transformations\": [],\n",
    "    \"scheduleParams\": schedule_params\n",
    "}\n",
    "flow_res = flow_conn.createFlow(\n",
    "    obj = flow_obj,\n",
    "    flow_spec_id = flow_spec_id\n",
    ")\n",
    "dataflow_id = flow_res[\"id\"]\n",
    "dataflow_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9dc178",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "If you would like to switch to a different cloud storage, you need to update the `flow_spec_id` variable above to the matching value in the table mentioned earlier in this section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff24ad",
   "metadata": {},
   "source": [
    "After you create the data flow, you should be able to see it in the UI to monitor executions, runtimes and its overall lifecycle. You can get the link below and should be able to see it in the UI as shown in the screenshot as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a9828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Flow created as ID 40a89499-90f1-4988-a38e-55b19d8ce091 available under https://experience.adobe.com/#/@aemonacpprodcampaign/sname:laa-e2e/platform/destination/browse/40a89499-90f1-4988-a38e-55b19d8ce091\n"
     ]
    }
   ],
   "source": [
    "from aepp import schema\n",
    "tenant_id = schema.Schema().getTenantId()\n",
    "\n",
    "dataflow_link = get_ui_link(tenant_id, \"destination/browse\", dataflow_id)\n",
    "print(f\"Data Flow created as ID {dataflow_id} available under {dataflow_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66a686",
   "metadata": {},
   "source": [
    "![Dataflow](./media/CMLE-Notebooks-Week2-Dataflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1bd09",
   "metadata": {},
   "source": [
    "# 4. Execute the Data Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ca293",
   "metadata": {},
   "source": [
    "At this point we've just created our Data Flow, but it has not executed yet. Please follow the instructions for the option you selected in Section 4.3 :\n",
    "- If you do not want to wait you can do a **adhoc run** to execute it instantly.\n",
    "- Either **wait until it gets scheduled**. We selected to have it run every 3 hours, so you may need to wait up to 3 hours.\n",
    "\n",
    "In the cell below we're showing how to do the first option to trigger a adhoc run, if you selected the second option, you can skip the cell below and will need to wait up to 3 hours to execute the cells after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff4e79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please wait at least 10 minutes after creating the dataflow before triggering the next cell, otherwise the job might not execute at all.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c7aec",
   "metadata": {},
   "source": [
    "### Trigger an ad hoc run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d71e029d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'destinations': [{'datasets': [{'id': '654b3ddd7dc4ee28d32c8c6e',\n",
       "     'statusURL': 'https://platform.adobe.io/data/foundation/flowservice/runs/e27898e4-7896-4ec9-b5cb-6ac13bc23a6c',\n",
       "     'flowId': '40a89499-90f1-4988-a38e-55b19d8ce091'}]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use new functionality in aepp when it is released\n",
    "from aepp import connector\n",
    "\n",
    "connector = connector.AdobeRequest(\n",
    "  config_object=aepp.config.config_object,\n",
    "  header=aepp.config.header,\n",
    "  loggingEnabled=False,\n",
    "  logger=None,\n",
    ")\n",
    "\n",
    "endpoint = aepp.config.endpoints[\"global\"] + \"/data/core/activation/disflowprovider/adhocrun\"\n",
    "\n",
    "payload = {\n",
    "    \"activationInfo\": {\n",
    "        \"destinations\": [\n",
    "            {\n",
    "                \"flowId\": dataflow_id, \n",
    "                \"datasets\": [\n",
    "                    {\"id\": created_dataset_id}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "connector.header.update({\"Accept\":\"application/vnd.adobe.adhoc.dataset.activation+json; version=1\"})\n",
    "activation_res = connector.postData(endpoint=endpoint, data=payload)\n",
    "activation_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a78b37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "If you see an error such as `Invalid parameter: Flow for id 93790efa-645b-4400-8afe-b6f135734656 is incorrect. Error is [Adhoc run can not be executed for Flow spec=cd2fc47e-e838-4f38-a581-8fff2f99b63a.`. it means your cloud storage is not yet whitelisted for exporting datasets. Please reach out to your Adobe contact to have it enabled.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d182b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "If you see an error such as `Invalid parameter: Following order ID(s) are not ready for dataset export, please wait for 10 minutes and retry.`. it means you need to wait a few minutes and retry again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033dee8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> \n",
    "\n",
    "If you get a message saying a run already exists, it means that either this dataset has been exported already based on the schedule, or that you've already done an adhoc export before.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e36552",
   "metadata": {},
   "source": [
    "Now we can check the execution of our Data Flow to make sure it actually executes. You can run the following cell until you can see the run appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "124fe9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No runs completed yet for flow 40a89499-90f1-4988-a38e-55b19d8ce091\n",
      "Run ID e27898e4-7896-4ec9-b5cb-6ac13bc23a6c completed with: duration=14.767 secs; size=3.0555238723754883 MB; num_rows=99923; num_files=12\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# TODO: handle that more gracefully in aepp\n",
    "finished = False\n",
    "while not finished:\n",
    "    try:\n",
    "        runs = flow_conn.getRuns(prop=f\"flowId=={dataflow_id}\")\n",
    "        for run in runs:\n",
    "            run_id = run[\"id\"]\n",
    "            run_started_at = run[\"metrics\"][\"durationSummary\"][\"startedAtUTC\"]\n",
    "            run_ended_at = run[\"metrics\"][\"durationSummary\"][\"completedAtUTC\"]\n",
    "            run_duration_secs = (run_ended_at - run_started_at) / 1000\n",
    "            run_size_mb = run[\"metrics\"][\"sizeSummary\"][\"outputBytes\"] / 1024. / 1024.\n",
    "            run_num_rows = run[\"metrics\"][\"recordSummary\"][\"outputRecordCount\"]\n",
    "            run_num_files = run[\"metrics\"][\"fileSummary\"][\"outputFileCount\"]\n",
    "            print(f\"Run ID {run_id} completed with: duration={run_duration_secs} secs; size={run_size_mb} MB; num_rows={run_num_rows}; num_files={run_num_files}\")\n",
    "        finished = True\n",
    "    except Exception as e:\n",
    "        print(f\"No runs completed yet for flow {dataflow_id}\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916beb77",
   "metadata": {},
   "source": [
    "Now that a run of our Data Flow has executed successfully, we're all set! We can do a sanity check to verify that the data indeed made its way into the DLZ. For that, we recommend setting up [Azure Storage Explorer](https://azure.microsoft.com/en-us/products/storage/storage-explorer) to connect to your DLZ container using [this guide](https://experienceleague.adobe.com/docs/experience-platform/destinations/catalog/cloud-storage/data-landing-zone.html?lang=en). To get the credentials, you can execute the code below to get the SAS URL needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44677122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLZ container: dlz-destination\n",
      "DLZ storage account: sndbxdtlndh8ia8em3oyoh69\n",
      "DLZ SAS URL: https://sndbxdtlndh8ia8em3oyoh69.blob.core.windows.net/dlz-destination?sv=2020-10-02&si=dlz-c9c43e14-26f9-4e2d-a1df-4e6f51e718f3&sr=c&sp=rl&sig=lU9zXuPnjqOeuheouud%2FKPBgi%2F4v4MbhUb%2B8Cv1xxKw%3D\n"
     ]
    }
   ],
   "source": [
    "# TODO: use functionality in aepp once released\n",
    "from aepp import connector\n",
    "\n",
    "connector = connector.AdobeRequest(\n",
    "  config_object=aepp.config.config_object,\n",
    "  header=aepp.config.header,\n",
    "  loggingEnabled=False,\n",
    "  logger=None,\n",
    ")\n",
    "\n",
    "endpoint = aepp.config.endpoints[\"global\"] + \"/data/foundation/connectors/landingzone/credentials\"\n",
    "\n",
    "dlz_credentials = connector.getData(endpoint=endpoint, params={\n",
    "  \"type\": \"dlz_destination\"\n",
    "})\n",
    "dlz_container = dlz_credentials[\"containerName\"]\n",
    "dlz_sas_token = dlz_credentials[\"SASToken\"]\n",
    "dlz_storage_account = dlz_credentials[\"storageAccountName\"]\n",
    "dlz_sas_uri = dlz_credentials[\"SASUri\"]\n",
    "print(f\"DLZ container: {dlz_container}\")\n",
    "print(f\"DLZ storage account: {dlz_storage_account}\")\n",
    "print(f\"DLZ SAS URL: {dlz_sas_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c528cd7",
   "metadata": {},
   "source": [
    "Once setup you should be able to see your featurized data as a set of Parquet files under the following directory structure: `cmle/egress/$DATASETID/exportTime=$TIMESTAMP` - see screenshot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73838de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurized data in DLZ should be available under cmle/egress/654b3ddd7dc4ee28d32c8c6e\n"
     ]
    }
   ],
   "source": [
    "print(f\"Featurized data in DLZ should be available under {export_path}/{created_dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca50e2",
   "metadata": {},
   "source": [
    "![DLZ](./media/CMLE-Notebooks-Week2-ExportedDataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fde872",
   "metadata": {},
   "source": [
    "### Next: Train a propensity model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
